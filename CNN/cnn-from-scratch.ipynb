{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-03-07T21:56:23.313623Z","iopub.status.busy":"2023-03-07T21:56:23.313156Z","iopub.status.idle":"2023-03-07T21:56:23.320534Z","shell.execute_reply":"2023-03-07T21:56:23.318865Z","shell.execute_reply.started":"2023-03-07T21:56:23.313584Z"}},"source":["# CNN from scratch\n","\n","This notebook builds a basic **Encoder-Decoder** variant of the Transformer architecture from scratch (Multi-Head Attention, Scaled Dot-Product Attention and Causal Masking included) in TensorFlow.\n","\n","It serves to understand how each part of the Transformer works and how they all fit together.\n","\n","The Transformer is then tested on a simple seq2seq task : **translating sentences from English to French**.\n","\n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n","\n","**Steps :**\n","1. [Preparing the data](#Preparing-the-data)\n","2. [Building the Transformer](#Building-the-Transformer)\n","3. [Training the Transformer](#Training-the-Transformer)\n","4. [Testing the Transformer](#Testing-the-Transformer)\n","    \n","*[Credits and stuff](#Credits-and-stuff)*"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:33:37.051559Z","iopub.status.busy":"2023-03-10T20:33:37.050322Z","iopub.status.idle":"2023-03-10T20:33:44.518521Z","shell.execute_reply":"2023-03-10T20:33:44.517328Z","shell.execute_reply.started":"2023-03-10T20:33:37.051514Z"},"trusted":true},"outputs":[],"source":["import os\n","from IPython.display import clear_output\n","import shutil\n","import pathlib\n","import pandas as pd\n","import random\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import string\n","import re\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:33:44.521811Z","iopub.status.busy":"2023-03-10T20:33:44.520984Z","iopub.status.idle":"2023-03-10T20:33:44.722362Z","shell.execute_reply":"2023-03-10T20:33:44.720969Z","shell.execute_reply.started":"2023-03-10T20:33:44.521768Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor Flow Version: 2.9.0\n","\n","GPU is AVAILABLE\n"]}],"source":["print(f\"Tensor Flow Version: {tf.__version__}\")\n","print()\n","gpu = len(tf.config.list_physical_devices('GPU'))>0\n","print(\"GPU is\", \"AVAILABLE\" if gpu else \"NOT AVAILABLE\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the data"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Reading the data</b></p>\n","</div>"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# get the files paths\n","input_dir_path = \"/kaggle/input\"\n","working_dir_path = \"/kaggle/working\"\n","input_dir_path = input_dir_path if os.path.exists(input_dir_path) else \".{}\".format(input_dir_path)\n","working_dir_path = working_dir_path if os.path.exists(working_dir_path) else \".{}\".format(working_dir_path)\n","\n","train_file_path = os.path.join(input_dir_path, \"dogs-vs-cats-redux-kernels-edition\", \"train.zip\")\n","test_file_path = os.path.join(input_dir_path, \"dogs-vs-cats-redux-kernels-edition\", \"test.zip\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mv: ./kaggle/working/train: No such file or directory\n","mv: ./kaggle/working/test: No such file or directory\n"]}],"source":["# unzip the files\n","!unzip -o $train_file_path -d $working_dir_path\n","!unzip -o $test_file_path -d $working_dir_path\n","\n","clear_output()\n","\n","# create raw/train and raw/test directories and move the files there\n","raw_dir_path = os.path.join(working_dir_path, \"raw\")\n","train_dir_path = os.path.join(raw_dir_path, \"train\")\n","test_dir_path = os.path.join(raw_dir_path, \"test\")\n","\n","os.makedirs(train_dir_path, exist_ok=True)\n","os.makedirs(test_dir_path, exist_ok=True)\n","\n","!mv $working_dir_path/train $raw_dir_path\n","!mv $working_dir_path/test $raw_dir_path"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/reno/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:3201: RuntimeWarning: invalid value encountered in divide\n","  x = x / sx\n"]},{"ename":"RuntimeError","evalue":"In affine_transform: Invalid vertices array.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m     axes[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39m\"\u001b[39m\u001b[39mdog\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m     plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m---> 53\u001b[0m display_pie_charts()\n\u001b[1;32m     55\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcat train count: \u001b[39m\u001b[39m{\u001b[39;00mtrain_cat_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcat val count: \u001b[39m\u001b[39m{\u001b[39;00mval_cat_count\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mdisplay_pie_charts\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdisplay_pie_charts\u001b[39m():\n\u001b[1;32m     44\u001b[0m     fig, axes \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, figsize\u001b[39m=\u001b[39m(\u001b[39m10\u001b[39m, \u001b[39m5\u001b[39m))\n\u001b[0;32m---> 45\u001b[0m     axes[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mpie([train_cat_count, val_cat_count], labels\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m\"\u001b[39;49m], autopct\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m%1.1f\u001b[39;49;00m\u001b[39m%%\u001b[39;49;00m\u001b[39m'\u001b[39;49m, shadow\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, startangle\u001b[39m=\u001b[39;49m\u001b[39m90\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m     axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39mequal\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m     axes[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mset_title(\u001b[39m\"\u001b[39m\u001b[39mcat\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/__init__.py:1442\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m   1440\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(ax, \u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1441\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1442\u001b[0m         \u001b[39mreturn\u001b[39;00m func(ax, \u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(sanitize_sequence, args), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1444\u001b[0m     bound \u001b[39m=\u001b[39m new_sig\u001b[39m.\u001b[39mbind(ax, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1445\u001b[0m     auto_label \u001b[39m=\u001b[39m (bound\u001b[39m.\u001b[39marguments\u001b[39m.\u001b[39mget(label_namer)\n\u001b[1;32m   1446\u001b[0m                   \u001b[39mor\u001b[39;00m bound\u001b[39m.\u001b[39mkwargs\u001b[39m.\u001b[39mget(label_namer))\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/axes/_axes.py:3259\u001b[0m, in \u001b[0;36mAxes.pie\u001b[0;34m(self, x, explode, labels, colors, autopct, pctdistance, shadow, labeldistance, startangle, radius, counterclock, wedgeprops, textprops, center, frame, rotatelabels, normalize, hatch)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[39mif\u001b[39;00m shadow:\n\u001b[1;32m   3256\u001b[0m     \u001b[39m# Make sure to add a shadow after the call to add_patch so the\u001b[39;00m\n\u001b[1;32m   3257\u001b[0m     \u001b[39m# figure and transform props will be set.\u001b[39;00m\n\u001b[1;32m   3258\u001b[0m     shad \u001b[39m=\u001b[39m mpatches\u001b[39m.\u001b[39mShadow(w, \u001b[39m-\u001b[39m\u001b[39m0.02\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m0.02\u001b[39m, label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m_nolegend_\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_patch(shad)\n\u001b[1;32m   3261\u001b[0m \u001b[39mif\u001b[39;00m labeldistance \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3262\u001b[0m     xt \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m labeldistance \u001b[39m*\u001b[39m radius \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mcos(thetam)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/axes/_base.py:2379\u001b[0m, in \u001b[0;36m_AxesBase.add_patch\u001b[0;34m(self, p)\u001b[0m\n\u001b[1;32m   2377\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mget_clip_path() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2378\u001b[0m     p\u001b[39m.\u001b[39mset_clip_path(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpatch)\n\u001b[0;32m-> 2379\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_patch_limits(p)\n\u001b[1;32m   2380\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mappend(p)\n\u001b[1;32m   2381\u001b[0m p\u001b[39m.\u001b[39m_remove_method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_children\u001b[39m.\u001b[39mremove\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/axes/_base.py:2421\u001b[0m, in \u001b[0;36m_AxesBase._update_patch_limits\u001b[0;34m(self, patch)\u001b[0m\n\u001b[1;32m   2419\u001b[0m         updatey \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   2420\u001b[0m trf_to_data \u001b[39m=\u001b[39m patch_trf \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransData\n\u001b[0;32m-> 2421\u001b[0m xys \u001b[39m=\u001b[39m trf_to_data\u001b[39m.\u001b[39;49mtransform(vertices)\n\u001b[1;32m   2422\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_datalim(xys, updatex\u001b[39m=\u001b[39mupdatex, updatey\u001b[39m=\u001b[39mupdatey)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/transforms.py:1779\u001b[0m, in \u001b[0;36mAffineBase.transform\u001b[0;34m(self, values)\u001b[0m\n\u001b[1;32m   1777\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, values):\n\u001b[1;32m   1778\u001b[0m     \u001b[39m# docstring inherited\u001b[39;00m\n\u001b[0;32m-> 1779\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_affine(values)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/transforms.py:1848\u001b[0m, in \u001b[0;36mAffine2DBase.transform_affine\u001b[0;34m(self, points)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     tpoints \u001b[39m=\u001b[39m affine_transform(points\u001b[39m.\u001b[39mdata, mtx)\n\u001b[1;32m   1847\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39mMaskedArray(tpoints, mask\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mma\u001b[39m.\u001b[39mgetmask(points))\n\u001b[0;32m-> 1848\u001b[0m \u001b[39mreturn\u001b[39;00m affine_transform(points, mtx)\n","\u001b[0;31mRuntimeError\u001b[0m: In affine_transform: Invalid vertices array."]},{"ename":"ValueError","evalue":"need at least one array to concatenate","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/IPython/core/formatters.py:338\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    339\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    340\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/backend_bases.py:2346\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2344\u001b[0m \u001b[39mif\u001b[39;00m bbox_inches:\n\u001b[1;32m   2345\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtight\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 2346\u001b[0m         bbox_inches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mget_tightbbox(\n\u001b[1;32m   2347\u001b[0m             renderer, bbox_extra_artists\u001b[39m=\u001b[39;49mbbox_extra_artists)\n\u001b[1;32m   2348\u001b[0m         \u001b[39mif\u001b[39;00m pad_inches \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2349\u001b[0m             pad_inches \u001b[39m=\u001b[39m rcParams[\u001b[39m'\u001b[39m\u001b[39msavefig.pad_inches\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/figure.py:1744\u001b[0m, in \u001b[0;36mFigureBase.get_tightbbox\u001b[0;34m(self, renderer, bbox_extra_artists)\u001b[0m\n\u001b[1;32m   1741\u001b[0m     artists \u001b[39m=\u001b[39m bbox_extra_artists\n\u001b[1;32m   1743\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m artists:\n\u001b[0;32m-> 1744\u001b[0m     bbox \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mget_tightbbox(renderer)\n\u001b[1;32m   1745\u001b[0m     \u001b[39mif\u001b[39;00m bbox \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1746\u001b[0m         bb\u001b[39m.\u001b[39mappend(bbox)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/axes/_base.py:4408\u001b[0m, in \u001b[0;36m_AxesBase.get_tightbbox\u001b[0;34m(self, renderer, call_axes_locator, bbox_extra_artists, for_layout_only)\u001b[0m\n\u001b[1;32m   4405\u001b[0m     bbox_artists \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_default_bbox_extra_artists()\n\u001b[1;32m   4407\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m bbox_artists:\n\u001b[0;32m-> 4408\u001b[0m     bbox \u001b[39m=\u001b[39m a\u001b[39m.\u001b[39;49mget_tightbbox(renderer)\n\u001b[1;32m   4409\u001b[0m     \u001b[39mif\u001b[39;00m (bbox \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   4410\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m bbox\u001b[39m.\u001b[39mwidth \u001b[39m<\u001b[39m np\u001b[39m.\u001b[39minf\n\u001b[1;32m   4411\u001b[0m             \u001b[39mand\u001b[39;00m \u001b[39m0\u001b[39m \u001b[39m<\u001b[39m bbox\u001b[39m.\u001b[39mheight \u001b[39m<\u001b[39m np\u001b[39m.\u001b[39minf):\n\u001b[1;32m   4412\u001b[0m         bb\u001b[39m.\u001b[39mappend(bbox)\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/artist.py:367\u001b[0m, in \u001b[0;36mArtist.get_tightbbox\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tightbbox\u001b[39m(\u001b[39mself\u001b[39m, renderer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    353\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[39m    Like `.Artist.get_window_extent`, but includes any clipping.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39m        The enclosing bounding box (in figure pixel coordinates).\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 367\u001b[0m     bbox \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_window_extent(renderer)\n\u001b[1;32m    368\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_on():\n\u001b[1;32m    369\u001b[0m         clip_box \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_clip_box()\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/patches.py:604\u001b[0m, in \u001b[0;36mPatch.get_window_extent\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_window_extent\u001b[39m(\u001b[39mself\u001b[39m, renderer\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 604\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_path()\u001b[39m.\u001b[39;49mget_extents(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_transform())\n","File \u001b[0;32m~/Documents/REPOS/Kaggle/.venv/lib/python3.10/site-packages/matplotlib/path.py:638\u001b[0m, in \u001b[0;36mPath.get_extents\u001b[0;34m(self, transform, **kwargs)\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[39m# as can the ends of the curve\u001b[39;00m\n\u001b[1;32m    637\u001b[0m         xys\u001b[39m.\u001b[39mappend(curve([\u001b[39m0\u001b[39m, \u001b[39m*\u001b[39mdzeros, \u001b[39m1\u001b[39m]))\n\u001b[0;32m--> 638\u001b[0m     xys \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(xys)\n\u001b[1;32m    639\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(xys):\n\u001b[1;32m    640\u001b[0m     \u001b[39mreturn\u001b[39;00m Bbox([xys\u001b[39m.\u001b[39mmin(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), xys\u001b[39m.\u001b[39mmax(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)])\n","File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"]},{"data":{"text/plain":["<Figure size 1000x500 with 2 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# count the number of jpg files in the train directory\n","train_dir_path = os.path.join(raw_dir_path, \"train\")\n","jpg_files = [f for f in os.listdir(train_dir_path) if f.endswith(\".jpg\")]\n","\n","cat_jpg_files = [f for f in jpg_files if f.startswith(\"cat\")]\n","dog_jpg_files = [f for f in jpg_files if f.startswith(\"dog\")]\n","\n","cat_train_size = int(len(cat_jpg_files) * 0.7)\n","cat_val_size = int(len(cat_jpg_files) * 0.3)\n","\n","dog_train_size = int(len(dog_jpg_files) * 0.6)\n","dog_val_size = int(len(dog_jpg_files) * 0.4)\n","\n","# structure the files so they can be loaded by tf.keras.preprocessing.image_dataset_from_directory\n","def make_subset(category, subset_name, start_index, end_index):\n","    dir_path = os.path.join(working_dir_path, subset_name, category)\n","    os.makedirs(dir_path)\n","    fnames = [f\"{category}.{i}.jpg\" \n","                for i in range(start_index, end_index)]\n","    for fname in fnames:\n","        src_path = os.path.join(train_dir_path, fname)\n","        dst_path = os.path.join(dir_path, fname)\n","        shutil.copyfile(src=src_path, dst=dst_path)\n","\n","make_subset(\"cat\", \"train\", start_index=0, end_index=cat_train_size)\n","make_subset(\"cat\", \"val\", start_index=cat_train_size, end_index=cat_train_size + cat_val_size)\n","make_subset(\"dog\", \"train\", start_index=0, end_index=dog_train_size)\n","make_subset(\"dog\", \"val\", start_index=dog_train_size, end_index=dog_train_size + dog_val_size)\n","\n","# delete the raw data\n","shutil.rmtree(raw_dir_path)\n","\n","# count each category in the train and validation directories\n","def count_categories(subset_name):\n","    cat_count = len(os.listdir(os.path.join(working_dir_path, subset_name, \"cat\")))\n","    dog_count = len(os.listdir(os.path.join(working_dir_path, subset_name, \"dog\")))\n","    return cat_count, dog_count\n","\n","train_cat_count, train_dog_count = count_categories(\"train\")\n","val_cat_count, val_dog_count = count_categories(\"val\")\n","\n","# display the pie charts side by side of cat's train count vs val count, and the same for dog's\n","def display_pie_charts():\n","    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n","    axes[0].pie([train_cat_count, val_cat_count], labels=[\"train\", \"val\"], autopct='%1.1f%%', shadow=True, startangle=90)\n","    axes[0].axis('equal')\n","    axes[0].set_title(\"cat\")\n","    axes[1].pie([train_dog_count, val_dog_count], labels=[\"train\", \"val\"], autopct='%1.1f%%', shadow=True, startangle=90)\n","    axes[1].axis('equal')\n","    axes[1].set_title(\"dog\")\n","    plt.show()\n","\n","display_pie_charts()\n","\n","print(f\"cat train count: {train_cat_count}\")\n","print(f\"cat val count: {val_cat_count}\")\n","print(f\"dog train count: {train_dog_count}\")\n","print(f\"dog val count: {val_dog_count}\")"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:33:44.724559Z","iopub.status.busy":"2023-03-10T20:33:44.723880Z","iopub.status.idle":"2023-03-10T20:33:45.215269Z","shell.execute_reply":"2023-03-10T20:33:45.214263Z","shell.execute_reply.started":"2023-03-10T20:33:44.724516Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>source</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>129522</th>\n","      <td>I hear you know how to speak French.</td>\n","      <td>[start] J'entends que tu sais parler français....</td>\n","    </tr>\n","    <tr>\n","      <th>54246</th>\n","      <td>I feel like throwing up.</td>\n","      <td>[start] J'ai envie de dégueuler. [end]</td>\n","    </tr>\n","    <tr>\n","      <th>144791</th>\n","      <td>It seems that they took the wrong train.</td>\n","      <td>[start] Il semblerait qu'elles aient pris le m...</td>\n","    </tr>\n","    <tr>\n","      <th>77946</th>\n","      <td>She plays the piano by ear.</td>\n","      <td>[start] Elle joue du piano à l'oreille. [end]</td>\n","    </tr>\n","    <tr>\n","      <th>44625</th>\n","      <td>We all sang in unison.</td>\n","      <td>[start] Nous avons tous chanté à l'unisson. [end]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          source  \\\n","129522      I hear you know how to speak French.   \n","54246                   I feel like throwing up.   \n","144791  It seems that they took the wrong train.   \n","77946                She plays the piano by ear.   \n","44625                     We all sang in unison.   \n","\n","                                                   target  \n","129522  [start] J'entends que tu sais parler français....  \n","54246              [start] J'ai envie de dégueuler. [end]  \n","144791  [start] Il semblerait qu'elles aient pris le m...  \n","77946       [start] Elle joue du piano à l'oreille. [end]  \n","44625   [start] Nous avons tous chanté à l'unisson. [end]  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# get the file path\n","path = \"/kaggle/input\"\n","path = path if os.path.exists(path) else \".{}\".format(path)\n","file_path = os.path.join(path, \"language-translation-englishfrench\", \"eng_-french.csv\")\n","\n","# read the data\n","df = pd.read_csv(file_path)\n","df['source'] = df['English words/sentences']\n","\n","# let's add an initial “seed” token ([start]) and a stop token ([end]) to each target sentence.\n","df['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\n","df = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n","\n","# display a few random samples\n","df.sample(5)"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Shuffling the data and splitting it into train, validation, and test sets</b></p>\n","</div>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:33:45.218528Z","iopub.status.busy":"2023-03-10T20:33:45.217235Z","iopub.status.idle":"2023-03-10T20:33:45.411849Z","shell.execute_reply":"2023-03-10T20:33:45.410721Z","shell.execute_reply.started":"2023-03-10T20:33:45.218485Z"},"trusted":true},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["# shuffle the data\n","df = df.sample(frac=1).reset_index(drop=True)\n","\n","# split the data into train, validation, and test sets\n","train_size = int(len(df) * 0.7)\n","val_size = int(len(df) * 0.2)\n","test_size = int(len(df) * 0.1)\n","\n","train_df = df[:train_size]\n","val_df = df[train_size:train_size+val_size]\n","test_df = df[train_size+val_size:]\n","\n","# display the data sets representations using a pie chart just to see the distribution of the data\n","labels = 'Train', 'Validation', 'Test'\n","sizes = [len(train_df), len(val_df), len(test_df)]\n","explode = (0.1, 0, 0)\n","fig1, ax1 = plt.subplots()\n","ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n","ax1.axis('equal')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Standardizing, tokenizing and indexing the data</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["First, we need to parse our raw text data and vectorize it.\n","\n","To keep things simple, we will first limit our vocabulary using the **max_tokens** parameter. We will also limit the length of each sample using the **sequence_length** parameter.\n","\n","Each sample (text input) will be standardized, tokenized by word, and then indexed by token.\n","\n","This will result in a matrix of vectors (input IDs) of shape \"(batch_size, **sequence_length**)\"."]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:33:45.416787Z","iopub.status.busy":"2023-03-10T20:33:45.415564Z","iopub.status.idle":"2023-03-10T20:34:07.345672Z","shell.execute_reply":"2023-03-10T20:34:07.344005Z","shell.execute_reply.started":"2023-03-10T20:33:45.416731Z"},"trusted":true},"outputs":[],"source":["max_tokens = 25000\n","sequence_length = 30\n","\n","# define a custom standardization function that convert to lowercase and strips all punctuations except \"[\" and \"]\" (so we can tell apart \"start\" from \"[start]\").\n","strip_chars = string.punctuation\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n"," \n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","# tokenize the data using our custom standardization function\n","source_vectorization = tf.keras.layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = tf.keras.layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1, # add +1 token to our target sentences since they'll be shifted right by 1 during training\n","    standardize=custom_standardization,\n",")\n","\n","# index all tokens in the source and target sentences\n","train_source_texts = train_df['source'].values\n","train_target_texts = train_df['target'].values\n","source_vectorization.adapt(train_source_texts)\n","target_vectorization.adapt(train_target_texts)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:07.347149Z","iopub.status.busy":"2023-03-10T20:34:07.346862Z","iopub.status.idle":"2023-03-10T20:34:10.610080Z","shell.execute_reply":"2023-03-10T20:34:10.609032Z","shell.execute_reply.started":"2023-03-10T20:34:07.347122Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Source texts (one random sample): Have you ever broken a bone?\n","Target texts (one random sample): [start] Vous êtes-vous jamais cassé un os ? [end]\n","Source vectors (one random sample): tf.Tensor(\n","[  18    3  199  766    6 3182    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0], shape=(30,), dtype=int64)\n","Target vectors (one random sample): tf.Tensor(\n","[   2   12  221   68  976   16 3233    3    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0], shape=(31,), dtype=int64)\n","Source decoded texts (one random sample): have you ever broken a bone                         \n","Target decoded texts (one random sample): [start] vous êtesvous jamais cassé un os [end]                        \n"]}],"source":["# display a random sample before and after vectorization just to test the vectorization\n","random_sample = random.randint(0, len(train_df))\n","print(\"Source texts (one random sample):\", train_source_texts[random_sample])\n","print(\"Target texts (one random sample):\", train_target_texts[random_sample])\n","print(\"Source vectors (one random sample):\", source_vectorization(train_source_texts[random_sample]))\n","print(\"Target vectors (one random sample):\", target_vectorization(train_target_texts[random_sample]))\n","\n","# display the decoding of the vectorized text (from vector back to text) just to test the vectorization\n","source_decoded_text = ''\n","for i in range(len(source_vectorization(train_source_texts[random_sample]))):\n","    source_decoded_text += source_vectorization.get_vocabulary()[source_vectorization(train_source_texts[random_sample])[i]] + ' '\n","print(\"Source decoded texts (one random sample):\", source_decoded_text)\n","\n","target_decoded_text = ''\n","for i in range(len(target_vectorization(train_target_texts[random_sample]))):\n","    target_decoded_text += target_vectorization.get_vocabulary()[target_vectorization(train_target_texts[random_sample])[i]] + ' '\n","print(\"Target decoded texts (one random sample):\", target_decoded_text)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:10.611845Z","iopub.status.busy":"2023-03-10T20:34:10.611463Z","iopub.status.idle":"2023-03-10T20:34:11.226518Z","shell.execute_reply":"2023-03-10T20:34:11.225295Z","shell.execute_reply.started":"2023-03-10T20:34:10.611807Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Source vectors (shape): (122934, 30)\n","Target vectors (shape): (122934, 31)\n"]}],"source":["# display the shape of our vectorized data\n","train_source_vectors = source_vectorization(train_source_texts)\n","train_target_vectors = target_vectorization(train_target_texts)\n","print(\"Source vectors (shape):\", train_source_vectors.shape)\n","print(\"Target vectors (shape):\", train_target_vectors.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Building the Transformer"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Positional Embedding</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["In order for the Transformer to be aware of the word order in each sentence, we must add some positional information to the data.\n","\n","First, each vector of tokens will be embedded in a low-dimensional vectors (the dimensionality of the embedding space is defined by the **embedding_size** parameter).\n","\n","Secondly, position information will be created and embedded, before being added to the embedded tokens.\n","\n","This will result in positional embeddings (position-aware word embeddings) stored in a matrix of shape \"(batch_size, sequence_length, **embedding_size**)\"."]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.228983Z","iopub.status.busy":"2023-03-10T20:34:11.228185Z","iopub.status.idle":"2023-03-10T20:34:11.239385Z","shell.execute_reply":"2023-03-10T20:34:11.238380Z","shell.execute_reply.started":"2023-03-10T20:34:11.228942Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim) # token embedding layer\n","        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=output_dim) # position embedding layer\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        embedded_tokens = self.token_embeddings(inputs) # embed the tokens\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1) # create the positional information\n","        embedded_positions = self.position_embeddings(positions) # embed the positions \n","        return embedded_tokens + embedded_positions # add the token and position embeddings to create the positional embeddings\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"input_dim\": self.input_dim,\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.241903Z","iopub.status.busy":"2023-03-10T20:34:11.241080Z","iopub.status.idle":"2023-03-10T20:34:11.357663Z","shell.execute_reply":"2023-03-10T20:34:11.356639Z","shell.execute_reply.started":"2023-03-10T20:34:11.241864Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Source embedded vectors (shape): (122934, 30, 256)\n","Target embedded vectors (shape): (122934, 30, 256)\n"]}],"source":["# display the shape of our embedded data just to test the class\n","\n","embed_dim = 256\n","\n","train_source_embedded = PositionalEmbedding(\n","    sequence_length=sequence_length,\n","    input_dim=max_tokens,\n","    output_dim=embed_dim,\n","    name=\"source_embedding\",\n",") (train_source_vectors)\n","\n","train_target_embedded = PositionalEmbedding(\n","    sequence_length=sequence_length,\n","    input_dim=max_tokens,\n","    output_dim=embed_dim,\n","    name=\"target_embedding\",\n",") (train_source_vectors)\n","\n","print(\"Source embedded vectors (shape):\", train_source_embedded.shape)\n","print(\"Target embedded vectors (shape):\", train_target_embedded.shape)"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Attention mechanism</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["A simple implementation of a Transformer's attention mechanism from scratch.\n","\n","- Causal Masking\n","- Scaled Dot-Product Attention\n","- Multi-Head Attention\n","\n","In practice, we could just use **tf.keras.layers.MultiHeadAttention** instead of building it from scratch. 😒"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n","    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Causal Masking</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["Let's define a Causal Masking function that will serve to mask the *future* tokens during training.\n","\n","*[Credits to OpenAI](https://github.com/openai/gpt-2/blob/master/src/model.py) for that one.*"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.361490Z","iopub.status.busy":"2023-03-10T20:34:11.360388Z","iopub.status.idle":"2023-03-10T20:34:11.370067Z","shell.execute_reply":"2023-03-10T20:34:11.368884Z","shell.execute_reply.started":"2023-03-10T20:34:11.361450Z"},"trusted":true},"outputs":[],"source":["def shape_list(x):\n","    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n","    static = x.shape.as_list()\n","    dynamic = tf.shape(x)\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n","\n","def attention_mask(nd, ns, *, dtype):\n","    \"\"\"1's in the lower triangle, counting from the lower right corner.\n","    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n","    \"\"\"\n","    i = tf.range(nd)[:,None]\n","    j = tf.range(ns)\n","    m = i >= j - ns + nd\n","    return tf.cast(m, dtype)\n","\n","def mask_attn_weights(w):\n","    # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n","    _, _, nd, ns = shape_list(w)\n","    b = attention_mask(nd, ns, dtype=w.dtype)\n","    b = tf.reshape(b, [1, 1, nd, ns])\n","    w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n","    return w"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.372264Z","iopub.status.busy":"2023-03-10T20:34:11.371779Z","iopub.status.idle":"2023-03-10T20:34:11.402103Z","shell.execute_reply":"2023-03-10T20:34:11.400936Z","shell.execute_reply.started":"2023-03-10T20:34:11.372229Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Masked attention weights: tf.Tensor(\n","[[[[ 6.2196958e-01 -1.0000000e+10 -1.0000000e+10 -1.0000000e+10\n","    -1.0000000e+10]\n","   [ 3.5770154e-01  2.3619473e-01 -1.0000000e+10 -1.0000000e+10\n","    -1.0000000e+10]\n","   [ 2.1580112e-01  3.4929395e-02  1.6679084e-01 -1.0000000e+10\n","    -1.0000000e+10]\n","   [ 4.9473345e-01  2.1166432e-01  3.2739556e-01  5.5245554e-01\n","    -1.0000000e+10]\n","   [ 5.0063765e-01  1.5066671e-01  6.9962549e-01  8.6920905e-01\n","     8.9490736e-01]]]], shape=(1, 1, 5, 5), dtype=float32)\n"]}],"source":["# display the causal masking of a random tensor just to test the function\n","random_tensor = tf.random.uniform(shape=(1, 1, 5, 5), minval=0, maxval=1, dtype=tf.float32)\n","print(\"Masked attention weights:\", mask_attn_weights(random_tensor))"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n","    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Scaled Dot-Product Attention</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["The scaled dot product attention is calculated as follows :\n","\n","***Attention(q, k, v) = softmax(qk^T / √(d_k))v***\n","\n","Where **q**, **k** and **v** are the query, key, and value matrices and **d_k** is the dimensionality of the key matrix.\n","\n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-19.png\" width=\"100\">](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.403936Z","iopub.status.busy":"2023-03-10T20:34:11.403511Z","iopub.status.idle":"2023-03-10T20:34:11.410760Z","shell.execute_reply":"2023-03-10T20:34:11.409404Z","shell.execute_reply.started":"2023-03-10T20:34:11.403900Z"},"trusted":true},"outputs":[],"source":["def scaled_dot_product_attention(q, k, v, use_causal_mask=False):\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scores = tf.matmul(q, k, transpose_b=True) # Matmul of Q and K\n","    scaled_scores = scores / tf.math.sqrt(d_k) # Scale\n","    if use_causal_mask:\n","        scaled_scores = mask_attn_weights(scaled_scores) # Mask (opt.)\n","    weights = tf.nn.softmax(scaled_scores, axis=-1) # SoftMax\n","    outputs = tf.matmul(weights, v) # Matmul of SoftMax and V\n","    return outputs"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:11.412927Z","iopub.status.busy":"2023-03-10T20:34:11.412309Z","iopub.status.idle":"2023-03-10T20:34:13.003133Z","shell.execute_reply":"2023-03-10T20:34:13.002029Z","shell.execute_reply.started":"2023-03-10T20:34:11.412891Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Scaled dot product attention (shape): (122934, 1, 30, 256)\n"]}],"source":["# display the shape of our attention output just to test the function\n","input = train_source_embedded\n","input = tf.expand_dims(input, axis=1)\n","print(\"Scaled dot product attention (shape):\", scaled_dot_product_attention(input, input, input, use_causal_mask=True).shape)"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n","    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Multi-Head Attention</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["The multi-head attention is calculated as follows :\n","\n","***MultiHead(q, k, v) = Concat(head_1, ..., head_h)W^O*** \n","\n","Where head_i is the i-th attention head, W^O is the output matrix, and **h** is the number of attention heads.\n","\n","***head_i = Attention(qW_i^q, kW_i^k, vW_i^v)***\n","\n","Where W_i^q, W_i^k, and W_i^v are the matrices for the i-th attention head.\n","    \n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-20.png\" width=\"200\">](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:13.007780Z","iopub.status.busy":"2023-03-10T20:34:13.007282Z","iopub.status.idle":"2023-03-10T20:34:13.019871Z","shell.execute_reply":"2023-03-10T20:34:13.018613Z","shell.execute_reply.started":"2023-03-10T20:34:13.007750Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, h, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.h = h\n","        if embed_dim % h != 0:\n","            raise ValueError(\n","                f\"dimension of the embedding space = {embed_dim} should be divisible by number of heads = {h}\"\n","            )\n","        self.q_linear = tf.keras.layers.Dense(embed_dim)\n","        self.k_linear = tf.keras.layers.Dense(embed_dim)\n","        self.v_linear = tf.keras.layers.Dense(embed_dim)\n","        self.concat_linear = tf.keras.layers.Dense(embed_dim)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, shape=(batch_size, -1, self.h, self.embed_dim // self.h))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def concat_heads(self, x, batch_size):\n","        x = tf.transpose(x, perm=[0, 2, 1, 3])\n","        return tf.reshape(x, (batch_size, -1, self.embed_dim))\n","\n","    def call(self, q, k, v, use_causal_mask=False):\n","        batch_size = tf.shape(k)[0]\n","        q = self.q_linear(q)\n","        k = self.k_linear(k)\n","        v = self.v_linear(v)\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","        attention = scaled_dot_product_attention(q, k, v, use_causal_mask)\n","        concat = self.concat_heads(attention, batch_size)\n","        concat = self.concat_linear(concat)\n","        return concat\n","\n","    def get_config(self):\n","        config = super(MultiHeadAttention, self).get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"h\": self.h,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Encoder</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_encoder-zoom.png\" width=\"400\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n","\n","The role of the Encoder is to process the source sequence. Here, no Causal Masking is needed : information is allowed to flow in both directions.\n","\n","The Encoder's a pretty generic module that ingests a sequence and learns to turn it into a more useful representation. It can also be used by itself (without the Decoder) for Natural Language Understanding (NLU) tasks like classification or named entity recognition (NER).\n","\n","In the Encoder's Multi-Head Self-Attention layer ([Global self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_global_self_attention_layer)), the **Source Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue."]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:13.022094Z","iopub.status.busy":"2023-03-10T20:34:13.021704Z","iopub.status.idle":"2023-03-10T20:34:13.032723Z","shell.execute_reply":"2023-03-10T20:34:13.031414Z","shell.execute_reply.started":"2023-03-10T20:34:13.022056Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n","        self.global_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.feed_forward = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n","             tf.keras.layers.Dense(embed_dim),]\n","        )\n","        \n","    def call(self, x):\n","        # Post layer normalization\n","        x = self.layer_norm_1(x + self.global_self_attention(q=x, k=x, v=x))\n","        x = self.layer_norm_2(x + self.feed_forward(x))\n","        return x\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Decoder</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_decoder-zoom.png\" width=\"500\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n","\n","The role of the Decoder is to look at the target sequence so far and predict the next token in the sequence.\n","\n","Contrary to the Encoder, the Decoder is made of two Attention layers. The first Attention layer does a similar job as the Encoder's sole Attention layer, with the important distinction that here, Causal Masking is enabled because, to correctly train our Transformer to predict the next token, we need to mask the *future* tokens. Meanwhile, The second Attention layer is much more straight-forward and basically just acts as a bridge that connects the Encoder to the Decoder.\n","\n","In the Decoder's first Attention layer, the Masked Multi-Head Self-Attention layer ([Causal self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer)), the **Target Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue. Like mentionned above, Causal Masking is enabled in this layer.\n","\n","In the Decoder's second Attention layer, the Encoder-Decoder Attention layer ([Cross attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_cross_attention_layer)), the **outputs of the Encoder** are being passed to the **K**ey and **V**alue parameters, with the **outputs of the Decoder's Masked Multi-Head Self-Attention layer** being passed to the **Q**uery parameter.\n"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:13.035037Z","iopub.status.busy":"2023-03-10T20:34:13.034205Z","iopub.status.idle":"2023-03-10T20:34:13.047005Z","shell.execute_reply":"2023-03-10T20:34:13.046027Z","shell.execute_reply.started":"2023-03-10T20:34:13.034998Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.causal_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.cross_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.feed_forward = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n","             tf.keras.layers.Dense(embed_dim),]\n","        )\n","        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_3 = tf.keras.layers.LayerNormalization()\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","    def call(self, x, context):\n","        # Post layer normalization\n","        x = self.layer_norm_1(x + self.causal_self_attention(q=x, k=x, v=x, use_causal_mask=True))\n","        x = self.layer_norm_2(x + self.cross_attention(q=x, k=context, v=context))\n","        x = self.layer_norm_3(x + self.feed_forward(x))\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n","    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Putting it all together</b></p>\n","</div>"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n","\n","We  turn our data into a *tf.data pipeline* that returns a tuple (Inputs, Outputs) where Inputs is a dict with two keys : **encoder_inputs** (the source sequence) and **decoder_inputs** (the target sequence), and Outputs is a single key : **decoder_outputs** (the target sentence \"shifted right\").\n","\n","During training, the fact that our Outputs are offset by one step ahead (\"shifted right\"), combined with the Causal Masking of the Decoder (Masked Multi-Head Attention layer), ensures that the\n","predictions for position *i* can depend only on the known outputs at positions less than *i* (no *future* token is seen).\n","\n","During inference, we'll generate one target token at a time and then feed it back into the Decoder so that it can predict the next token. And so on."]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:13.048967Z","iopub.status.busy":"2023-03-10T20:34:13.048637Z","iopub.status.idle":"2023-03-10T20:34:13.320084Z","shell.execute_reply":"2023-03-10T20:34:13.319046Z","shell.execute_reply.started":"2023-03-10T20:34:13.048927Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","\n","def format_dataset(source, target):\n","    source_vectors = source_vectorization(source)\n","    target_vectors = target_vectorization(target)\n","    return ({\n","        \"source\": source_vectors, # encoder_inputs\n","        \"target\": target_vectors[:, :-1], # decoder_inputs (truncate by 1 to keep it at the same length as decoder_outputs, which is shifted right by 1).\n","    }, target_vectors[:, 1:]) # decoder_outputs\n","\n","def make_dataset(df):\n","    dataset = tf.data.Dataset.from_tensor_slices((df[\"source\"].values, df[\"target\"].values))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train_df)\n","val_ds = make_dataset(val_df)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:13.322206Z","iopub.status.busy":"2023-03-10T20:34:13.321840Z","iopub.status.idle":"2023-03-10T20:34:14.086482Z","shell.execute_reply":"2023-03-10T20:34:14.085406Z","shell.execute_reply.started":"2023-03-10T20:34:13.322169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Encoder Inputs: (64, 30)\n","Decoder Inputs: (64, 30)\n","Decoder Outputs: (64, 30)\n"]}],"source":["# display the shape of the first batch of data in the dataset just to see what it looks like\n","for batch in train_ds.take(1):\n","    print(\"Encoder Inputs:\", batch[0][\"source\"].shape)\n","    print(\"Decoder Inputs:\", batch[0][\"target\"].shape)\n","    print(\"Decoder Outputs:\", batch[1].shape)"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_transformer-encoder-decoder.png\" width=\"600\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:14.088162Z","iopub.status.busy":"2023-03-10T20:34:14.087822Z","iopub.status.idle":"2023-03-10T20:34:15.527732Z","shell.execute_reply":"2023-03-10T20:34:15.526626Z","shell.execute_reply.started":"2023-03-10T20:34:14.088129Z"},"trusted":true},"outputs":[],"source":["embed_dim = 512 # dimension of the embedding space\n","dense_dim = 2048 # dimension of the feed forward network (a rule of thumb is to use 4 times the size of the embeddings)\n","num_heads = 8\n","\n","# the transformer body\n","encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n","x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\n","x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","\n","# the transformer head\n","x = tf.keras.layers.Dropout(0.5)(x)\n","decoder_outputs = tf.keras.layers.Dense(max_tokens, activation=\"softmax\")(x)\n","\n","transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training the Transformer"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T20:34:15.529905Z","iopub.status.busy":"2023-03-10T20:34:15.529276Z","iopub.status.idle":"2023-03-10T21:39:20.048842Z","shell.execute_reply":"2023-03-10T21:39:20.047379Z","shell.execute_reply.started":"2023-03-10T20:34:15.529864Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50\n","1921/1921 [==============================] - 235s 117ms/step - loss: 1.0998 - accuracy: 0.8344 - val_loss: 0.6480 - val_accuracy: 0.8921 - lr: 0.0010\n","Epoch 2/50\n","1921/1921 [==============================] - 210s 109ms/step - loss: 0.6043 - accuracy: 0.8979 - val_loss: 0.5138 - val_accuracy: 0.9082 - lr: 0.0010\n","Epoch 3/50\n","1921/1921 [==============================] - 210s 109ms/step - loss: 0.4871 - accuracy: 0.9120 - val_loss: 0.4610 - val_accuracy: 0.9160 - lr: 0.0010\n","Epoch 4/50\n","1921/1921 [==============================] - 210s 109ms/step - loss: 0.4219 - accuracy: 0.9203 - val_loss: 0.4390 - val_accuracy: 0.9199 - lr: 0.0010\n","Epoch 5/50\n","1921/1921 [==============================] - 209s 109ms/step - loss: 0.3768 - accuracy: 0.9264 - val_loss: 0.4298 - val_accuracy: 0.9219 - lr: 0.0010\n","Epoch 6/50\n","1921/1921 [==============================] - 209s 109ms/step - loss: 0.3412 - accuracy: 0.9316 - val_loss: 0.4254 - val_accuracy: 0.9242 - lr: 0.0010\n","Epoch 7/50\n","1921/1921 [==============================] - 208s 108ms/step - loss: 0.3125 - accuracy: 0.9359 - val_loss: 0.4301 - val_accuracy: 0.9238 - lr: 0.0010\n","Epoch 8/50\n","1921/1921 [==============================] - 208s 108ms/step - loss: 0.2882 - accuracy: 0.9397 - val_loss: 0.4279 - val_accuracy: 0.9255 - lr: 0.0010\n","Epoch 9/50\n","1921/1921 [==============================] - 208s 108ms/step - loss: 0.2665 - accuracy: 0.9432 - val_loss: 0.4277 - val_accuracy: 0.9264 - lr: 0.0010\n","Epoch 10/50\n","1921/1921 [==============================] - 209s 109ms/step - loss: 0.1854 - accuracy: 0.9569 - val_loss: 0.3639 - val_accuracy: 0.9345 - lr: 1.0000e-04\n","Epoch 11/50\n","1921/1921 [==============================] - 209s 109ms/step - loss: 0.1564 - accuracy: 0.9622 - val_loss: 0.3598 - val_accuracy: 0.9351 - lr: 1.0000e-04\n","Epoch 12/50\n","1921/1921 [==============================] - 209s 109ms/step - loss: 0.1436 - accuracy: 0.9646 - val_loss: 0.3595 - val_accuracy: 0.9354 - lr: 1.0000e-04\n","Epoch 13/50\n","1921/1921 [==============================] - 214s 111ms/step - loss: 0.1342 - accuracy: 0.9664 - val_loss: 0.3604 - val_accuracy: 0.9357 - lr: 1.0000e-04\n","Epoch 14/50\n","1921/1921 [==============================] - 221s 115ms/step - loss: 0.1268 - accuracy: 0.9679 - val_loss: 0.3612 - val_accuracy: 0.9357 - lr: 1.0000e-04\n","Epoch 15/50\n","1921/1921 [==============================] - 221s 115ms/step - loss: 0.1209 - accuracy: 0.9691 - val_loss: 0.3624 - val_accuracy: 0.9357 - lr: 1.0000e-04\n","Epoch 16/50\n","1921/1921 [==============================] - 223s 116ms/step - loss: 0.1122 - accuracy: 0.9709 - val_loss: 0.3606 - val_accuracy: 0.9359 - lr: 1.0000e-05\n","Epoch 17/50\n","1921/1921 [==============================] - 229s 119ms/step - loss: 0.1104 - accuracy: 0.9713 - val_loss: 0.3602 - val_accuracy: 0.9360 - lr: 1.0000e-05\n","Epoch 18/50\n","1921/1921 [==============================] - 224s 116ms/step - loss: 0.1095 - accuracy: 0.9715 - val_loss: 0.3604 - val_accuracy: 0.9361 - lr: 1.0000e-05\n"]},{"data":{"text/plain":["<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fe8a812d390>"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","EPOCHS = 50\n","checkpoint_filepath = '/tmp/checkpoint/'\n","callbacks_list = [\n","    tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor='val_loss',\n","        factor=0.1,\n","        patience=3,\n","    ),\n","    tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=6,\n","    ),\n","    tf.keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_filepath,\n","        save_weights_only=True,\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True\n","    ),\n","]\n","    \n","transformer.fit(train_ds, \n","                epochs=EPOCHS, \n","                callbacks=callbacks_list,\n","                validation_data=val_ds)\n","\n","transformer.load_weights(checkpoint_filepath)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing the Transformer"]},{"cell_type":"markdown","metadata":{},"source":["Let's translate a few random test sentences with our newly-trained Transformer."]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-03-10T21:39:20.051564Z","iopub.status.busy":"2023-03-10T21:39:20.051007Z","iopub.status.idle":"2023-03-10T21:39:53.506563Z","shell.execute_reply":"2023-03-10T21:39:53.504615Z","shell.execute_reply.started":"2023-03-10T21:39:20.051511Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["I want a martini.\n","[start] je veux un [UNK] [end]\n","\n","I'd like to introduce you to my wife.\n","[start] jaimerais te présenter ma femme [end]\n","\n","He is used to speaking in public.\n","[start] il a lhabitude de parler en public [end]\n","\n","He complained about the noise.\n","[start] il sest plaint du bruit [end]\n","\n","I'm real careful.\n","[start] je suis vraiment prudent [end]\n","\n","My French is actually not so good.\n","[start] mon français nest en réalité pas si bon [end]\n","\n","Don't think about the price.\n","[start] ny pense pas le prix [end]\n","\n","How long do they wish to spend here?\n","[start] combien de temps ils veulent passer ici [end]\n","\n","Do you see anything?\n","[start] estce que tu vois quelque chose [end]\n","\n","The argument is rigorous and coherent but ultimately unconvincing.\n","[start] le argument est en train et [UNK] mais un [UNK] [end]\n","\n","Let's put up the Christmas tree here.\n","[start] mettons un sapin de noël ici [end]\n","\n","She has two brothers, who work in the computer industry.\n","[start] elle a deux frères qui travaille dans lindustrie informatique [end]\n","\n","I'll go first.\n","[start] jirai en premier [end]\n","\n","What kind of meal did you eat?\n","[start] quel genre de repas avezvous mangé [end]\n","\n","He couldn't go out because of the snow.\n","[start] il ne pouvait sortir à cause de la neige [end]\n","\n","I am married and I have two sons.\n","[start] je suis marié et jai deux fils [end]\n","\n","Do you have a theory?\n","[start] avezvous une théorie [end]\n","\n","It's so easy.\n","[start] cest tellement facile [end]\n","\n","Enjoy your day.\n","[start] faites un jour [end]\n","\n","The place where he lives is far from town.\n","[start] lendroit où il vit à distance en ville [end]\n","\n","The mail can't be delivered.\n","[start] le courrier ne peut pas être distribué [end]\n","\n","That's what everyone's saying.\n","[start] cest ce que tout le monde dit [end]\n","\n","I read all kinds of books.\n","[start] jai tout lu sortes de livres [end]\n","\n","Are all of them your friends?\n","[start] tous vos amis [end]\n","\n","The party was held on May 22nd.\n","[start] la fête sest tenue à un [UNK] [end]\n","\n","I'm afraid we have no choice.\n","[start] je crains que nous nayons non [end]\n","\n","My job is to anticipate problems.\n","[start] mon travail est de [UNK] aux problèmes [end]\n","\n","Don't be so outraged.\n","[start] ne soyez pas si indignée [end]\n","\n","You should eat.\n","[start] tu devrais manger [end]\n","\n","Are you tall?\n","[start] Êtesvous grande [end]\n","\n","Tom fell down the stairs.\n","[start] tom est tombé dans les escaliers [end]\n","\n","Don't dwell on your past mistakes!\n","[start] ne ressasse pas tes erreurs passées [end]\n","\n","You're a horrible driver.\n","[start] vous avez une horrible conducteur [end]\n","\n","I don't think you would do that.\n","[start] je ne pense pas que tu ferais ça [end]\n","\n","I am going to buy a new car.\n","[start] je vais acheter une nouvelle voiture [end]\n","\n","She is used to cooking.\n","[start] elle est habituée à cuisiner [end]\n","\n","He likes vegetables, especially cabbage.\n","[start] il aime particulièrement les légumes particulièrement [end]\n","\n","I had a very high fever.\n","[start] jai eu une très haut de fièvre [end]\n","\n","He has enough ability to manage a business.\n","[start] il a suffisamment capacités pour faire un travail [end]\n","\n","He laid down his pen and leaned back in his chair.\n","[start] il [UNK] son stylo et dans le dos [end]\n","\n","It's more complicated than I originally thought.\n","[start] cest plus compliqué que moi je pensais [end]\n","\n","I don't know what I'm doing here.\n","[start] je ne sais pas ce que je fais ici [end]\n","\n","That movie was pretty boring.\n","[start] ce film était assez ennuyeux [end]\n","\n","He can't come with us.\n","[start] il ne peut pas venir avec nous [end]\n","\n","I've found a new job.\n","[start] jai trouvé un nouveau travail [end]\n","\n","I'm not presentable.\n","[start] je ne suis pas au courant [end]\n","\n","That baby is fat and healthy.\n","[start] ce bébé est gros et en bonne santé [end]\n","\n","He doesn't have his feet on the ground.\n","[start] il ne dispose pas de ses pieds sur le sol [end]\n","\n","I did that voluntarily.\n","[start] jai fait ça volontairement [end]\n","\n","My short-term memory is getting shorter and shorter.\n","[start] mon [UNK] est de la mémoire court et plus court [end]\n","\n"]}],"source":["target_vocab = target_vectorization.get_vocabulary()\n","target_index_lookup = dict(zip(range(len(target_vocab)), target_vocab))\n","max_decoded_sentence_length = 30\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = target_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","# let's translate 50 random sentences\n","for i in range(50):\n","    random_index = np.random.randint(0, len(test_df))\n","    input_sentence = test_df[\"source\"].iloc[random_index]\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))\n","    print()"]},{"cell_type":"markdown","metadata":{},"source":["# Credits and stuff\n","\n","- https://arxiv.org/abs/1706.03762\n","\n","- https://www.tensorflow.org/text/tutorials/transformer\n","\n","- https://github.com/openai/gpt-2/blob/master/src/model.py\n","\n","- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n","\n","- https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11\n","\n","    - https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n","\n","    - https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n","\n","- https://www.oreilly.com/library/view/natural-language-processing/9781098136789/ch03.html\n","\n","    - https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"1c472d3f56e1d3981a3fd37d3eee44d1ccdd21bd4124bc6c2c96c1c4dcfe0833"}}},"nbformat":4,"nbformat_minor":4}
