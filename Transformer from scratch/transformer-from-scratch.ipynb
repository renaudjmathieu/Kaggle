{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Transformer from scratch\n\nThis notebook builds a basic **Encoder-Decoder** variant of the Transformer architecture from scratch (Multi-Head Attention, Scaled Dot-Product Attention and Causal Masking included) in TensorFlow.\n\nIt serves to understand how each part of the Transformer works and how they all fit together.\n\nThe Transformer is then tested on a simple seq2seq task : **translating sentences from English to French**.\n\n[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n\n**Steps :**\n1. [Preparing the data](#Preparing-the-data)\n2. [Building the Transformer](#Building-the-Transformer)\n3. [Training the Transformer](#Training-the-Transformer)\n4. [Testing the Transformer](#Testing-the-Transformer)\n    \n*[Credits and stuff](#Credits-and-stuff)*","metadata":{"execution":{"iopub.execute_input":"2023-03-07T21:56:23.313623Z","iopub.status.busy":"2023-03-07T21:56:23.313156Z","iopub.status.idle":"2023-03-07T21:56:23.320534Z","shell.execute_reply":"2023-03-07T21:56:23.318865Z","shell.execute_reply.started":"2023-03-07T21:56:23.313584Z"}}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport string\nimport re\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:33:37.050322Z","iopub.execute_input":"2023-03-10T20:33:37.051559Z","iopub.status.idle":"2023-03-10T20:33:44.518521Z","shell.execute_reply.started":"2023-03-10T20:33:37.051514Z","shell.execute_reply":"2023-03-10T20:33:44.517328Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"print(f\"Tensor Flow Version: {tf.__version__}\")\nprint()\ngpu = len(tf.config.list_physical_devices('GPU'))>0\nprint(\"GPU is\", \"AVAILABLE\" if gpu else \"NOT AVAILABLE\")","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:33:44.520984Z","iopub.execute_input":"2023-03-10T20:33:44.521811Z","iopub.status.idle":"2023-03-10T20:33:44.722362Z","shell.execute_reply.started":"2023-03-10T20:33:44.521768Z","shell.execute_reply":"2023-03-10T20:33:44.720969Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Tensor Flow Version: 2.11.0\n\nGPU is AVAILABLE\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preparing the data","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Reading the data</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# get the file path\npath = \"/kaggle/input\"\npath = path if os.path.exists(path) else \".{}\".format(path)\nfile_path = os.path.join(path, \"language-translation-englishfrench\", \"eng_-french.csv\")\n\n# read the data\ndf = pd.read_csv(file_path)\ndf['source'] = df['English words/sentences']\n\n# let's add an initial “seed” token ([start]) and a stop token ([end]) to each target sentence.\ndf['target'] = df['French words/sentences'].apply(lambda x: '[start] ' + x + ' [end]')\ndf = df.drop(['English words/sentences', 'French words/sentences'], axis=1)\n\n# display a few random samples\ndf.sample(5)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:33:44.723880Z","iopub.execute_input":"2023-03-10T20:33:44.724559Z","iopub.status.idle":"2023-03-10T20:33:45.215269Z","shell.execute_reply.started":"2023-03-10T20:33:44.724516Z","shell.execute_reply":"2023-03-10T20:33:45.214263Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                          source  \\\n129522      I hear you know how to speak French.   \n54246                   I feel like throwing up.   \n144791  It seems that they took the wrong train.   \n77946                She plays the piano by ear.   \n44625                     We all sang in unison.   \n\n                                                   target  \n129522  [start] J'entends que tu sais parler français....  \n54246              [start] J'ai envie de dégueuler. [end]  \n144791  [start] Il semblerait qu'elles aient pris le m...  \n77946       [start] Elle joue du piano à l'oreille. [end]  \n44625   [start] Nous avons tous chanté à l'unisson. [end]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>source</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>129522</th>\n      <td>I hear you know how to speak French.</td>\n      <td>[start] J'entends que tu sais parler français....</td>\n    </tr>\n    <tr>\n      <th>54246</th>\n      <td>I feel like throwing up.</td>\n      <td>[start] J'ai envie de dégueuler. [end]</td>\n    </tr>\n    <tr>\n      <th>144791</th>\n      <td>It seems that they took the wrong train.</td>\n      <td>[start] Il semblerait qu'elles aient pris le m...</td>\n    </tr>\n    <tr>\n      <th>77946</th>\n      <td>She plays the piano by ear.</td>\n      <td>[start] Elle joue du piano à l'oreille. [end]</td>\n    </tr>\n    <tr>\n      <th>44625</th>\n      <td>We all sang in unison.</td>\n      <td>[start] Nous avons tous chanté à l'unisson. [end]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Shuffling the data and splitting it into train, validation, and test sets</b></p>\n</div>","metadata":{}},{"cell_type":"code","source":"# shuffle the data\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# split the data into train, validation, and test sets\ntrain_size = int(len(df) * 0.7)\nval_size = int(len(df) * 0.2)\ntest_size = int(len(df) * 0.1)\n\ntrain_df = df[:train_size]\nval_df = df[train_size:train_size+val_size]\ntest_df = df[train_size+val_size:]\n\n# display the data sets representations using a pie chart just to see the distribution of the data\nlabels = 'Train', 'Validation', 'Test'\nsizes = [len(train_df), len(val_df), len(test_df)]\nexplode = (0.1, 0, 0)\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\nax1.axis('equal')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:33:45.217235Z","iopub.execute_input":"2023-03-10T20:33:45.218528Z","iopub.status.idle":"2023-03-10T20:33:45.411849Z","shell.execute_reply.started":"2023-03-10T20:33:45.218485Z","shell.execute_reply":"2023-03-10T20:33:45.410721Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Standardizing, tokenizing and indexing the data</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"First, we need to parse our raw text data and vectorize it.\n\nTo keep things simple, we will first limit our vocabulary using the **max_tokens** parameter. We will also limit the length of each sample using the **sequence_length** parameter.\n\nEach sample (text input) will be standardized, tokenized by word, and then indexed by token.\n\nThis will result in a matrix of vectors (input IDs) of shape \"(batch_size, **sequence_length**)\".","metadata":{}},{"cell_type":"code","source":"max_tokens = 25000\nsequence_length = 30\n\n# define a custom standardization function that convert to lowercase and strips all punctuations except \"[\" and \"]\" (so we can tell apart \"start\" from \"[start]\").\nstrip_chars = string.punctuation\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n \ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(\n        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n\n# tokenize the data using our custom standardization function\nsource_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\ntarget_vectorization = tf.keras.layers.TextVectorization(\n    max_tokens=max_tokens,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1, # add +1 token to our target sentences since they'll be shifted right by 1 during training\n    standardize=custom_standardization,\n)\n\n# index all tokens in the source and target sentences\ntrain_source_texts = train_df['source'].values\ntrain_target_texts = train_df['target'].values\nsource_vectorization.adapt(train_source_texts)\ntarget_vectorization.adapt(train_target_texts)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:33:45.415564Z","iopub.execute_input":"2023-03-10T20:33:45.416787Z","iopub.status.idle":"2023-03-10T20:34:07.345672Z","shell.execute_reply.started":"2023-03-10T20:33:45.416731Z","shell.execute_reply":"2023-03-10T20:34:07.344005Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# display a random sample before and after vectorization just to test the vectorization\nrandom_sample = random.randint(0, len(train_df))\nprint(\"Source texts (one random sample):\", train_source_texts[random_sample])\nprint(\"Target texts (one random sample):\", train_target_texts[random_sample])\nprint(\"Source vectors (one random sample):\", source_vectorization(train_source_texts[random_sample]))\nprint(\"Target vectors (one random sample):\", target_vectorization(train_target_texts[random_sample]))\n\n# display the decoding of the vectorized text (from vector back to text) just to test the vectorization\nsource_decoded_text = ''\nfor i in range(len(source_vectorization(train_source_texts[random_sample]))):\n    source_decoded_text += source_vectorization.get_vocabulary()[source_vectorization(train_source_texts[random_sample])[i]] + ' '\nprint(\"Source decoded texts (one random sample):\", source_decoded_text)\n\ntarget_decoded_text = ''\nfor i in range(len(target_vectorization(train_target_texts[random_sample]))):\n    target_decoded_text += target_vectorization.get_vocabulary()[target_vectorization(train_target_texts[random_sample])[i]] + ' '\nprint(\"Target decoded texts (one random sample):\", target_decoded_text)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:07.346862Z","iopub.execute_input":"2023-03-10T20:34:07.347149Z","iopub.status.idle":"2023-03-10T20:34:10.610080Z","shell.execute_reply.started":"2023-03-10T20:34:07.347122Z","shell.execute_reply":"2023-03-10T20:34:10.609032Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Source texts (one random sample): Have you ever broken a bone?\nTarget texts (one random sample): [start] Vous êtes-vous jamais cassé un os ? [end]\nSource vectors (one random sample): tf.Tensor(\n[  18    3  199  766    6 3182    0    0    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0], shape=(30,), dtype=int64)\nTarget vectors (one random sample): tf.Tensor(\n[   2   12  221   68  976   16 3233    3    0    0    0    0    0    0\n    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n    0    0    0], shape=(31,), dtype=int64)\nSource decoded texts (one random sample): have you ever broken a bone                         \nTarget decoded texts (one random sample): [start] vous êtesvous jamais cassé un os [end]                        \n","output_type":"stream"}]},{"cell_type":"code","source":"# display the shape of our vectorized data\ntrain_source_vectors = source_vectorization(train_source_texts)\ntrain_target_vectors = target_vectorization(train_target_texts)\nprint(\"Source vectors (shape):\", train_source_vectors.shape)\nprint(\"Target vectors (shape):\", train_target_vectors.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:10.611463Z","iopub.execute_input":"2023-03-10T20:34:10.611845Z","iopub.status.idle":"2023-03-10T20:34:11.226518Z","shell.execute_reply.started":"2023-03-10T20:34:10.611807Z","shell.execute_reply":"2023-03-10T20:34:11.225295Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Source vectors (shape): (122934, 30)\nTarget vectors (shape): (122934, 31)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Building the Transformer","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Positional Embedding</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"In order for the Transformer to be aware of the word order in each sentence, we must add some positional information to the data.\n\nFirst, each vector of tokens will be embedded in a low-dimensional vectors (the dimensionality of the embedding space is defined by the **embedding_size** parameter).\n\nSecondly, position information will be created and embedded, before being added to the embedded tokens.\n\nThis will result in positional embeddings (position-aware word embeddings) stored in a matrix of shape \"(batch_size, sequence_length, **embedding_size**)\".","metadata":{}},{"cell_type":"code","source":"class PositionalEmbedding(tf.keras.layers.Layer):\n    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim) # token embedding layer\n        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=output_dim) # position embedding layer\n        self.sequence_length = sequence_length\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n\n    def call(self, inputs):\n        embedded_tokens = self.token_embeddings(inputs) # embed the tokens\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1) # create the positional information\n        embedded_positions = self.position_embeddings(positions) # embed the positions \n        return embedded_tokens + embedded_positions # add the token and position embeddings to create the positional embeddings\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super(PositionalEmbedding, self).get_config()\n        config.update({\n            \"input_dim\": self.input_dim,\n            \"output_dim\": self.output_dim,\n            \"sequence_length\": self.sequence_length,\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.228185Z","iopub.execute_input":"2023-03-10T20:34:11.228983Z","iopub.status.idle":"2023-03-10T20:34:11.239385Z","shell.execute_reply.started":"2023-03-10T20:34:11.228942Z","shell.execute_reply":"2023-03-10T20:34:11.238380Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# display the shape of our embedded data just to test the class\n\nembed_dim = 256\n\ntrain_source_embedded = PositionalEmbedding(\n    sequence_length=sequence_length,\n    input_dim=max_tokens,\n    output_dim=embed_dim,\n    name=\"source_embedding\",\n) (train_source_vectors)\n\ntrain_target_embedded = PositionalEmbedding(\n    sequence_length=sequence_length,\n    input_dim=max_tokens,\n    output_dim=embed_dim,\n    name=\"target_embedding\",\n) (train_source_vectors)\n\nprint(\"Source embedded vectors (shape):\", train_source_embedded.shape)\nprint(\"Target embedded vectors (shape):\", train_target_embedded.shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.241080Z","iopub.execute_input":"2023-03-10T20:34:11.241903Z","iopub.status.idle":"2023-03-10T20:34:11.357663Z","shell.execute_reply.started":"2023-03-10T20:34:11.241864Z","shell.execute_reply":"2023-03-10T20:34:11.356639Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Source embedded vectors (shape): (122934, 30, 256)\nTarget embedded vectors (shape): (122934, 30, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Attention mechanism</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"A simple implementation of a Transformer's attention mechanism from scratch.\n\n- Causal Masking\n- Scaled Dot-Product Attention\n- Multi-Head Attention\n\nIn practice, we could just use **tf.keras.layers.MultiHeadAttention** instead of building it from scratch. 😒","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Causal Masking</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"Let's define a Causal Masking function that will serve to mask the *future* tokens during training.\n\n*[Credits to OpenAI](https://github.com/openai/gpt-2/blob/master/src/model.py) for that one.*","metadata":{}},{"cell_type":"code","source":"def shape_list(x):\n    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n\ndef attention_mask(nd, ns, *, dtype):\n    \"\"\"1's in the lower triangle, counting from the lower right corner.\n    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n    \"\"\"\n    i = tf.range(nd)[:,None]\n    j = tf.range(ns)\n    m = i >= j - ns + nd\n    return tf.cast(m, dtype)\n\ndef mask_attn_weights(w):\n    # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n    _, _, nd, ns = shape_list(w)\n    b = attention_mask(nd, ns, dtype=w.dtype)\n    b = tf.reshape(b, [1, 1, nd, ns])\n    w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n    return w","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.360388Z","iopub.execute_input":"2023-03-10T20:34:11.361490Z","iopub.status.idle":"2023-03-10T20:34:11.370067Z","shell.execute_reply.started":"2023-03-10T20:34:11.361450Z","shell.execute_reply":"2023-03-10T20:34:11.368884Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# display the causal masking of a random tensor just to test the function\nrandom_tensor = tf.random.uniform(shape=(1, 1, 5, 5), minval=0, maxval=1, dtype=tf.float32)\nprint(\"Masked attention weights:\", mask_attn_weights(random_tensor))","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.371779Z","iopub.execute_input":"2023-03-10T20:34:11.372264Z","iopub.status.idle":"2023-03-10T20:34:11.402103Z","shell.execute_reply.started":"2023-03-10T20:34:11.372229Z","shell.execute_reply":"2023-03-10T20:34:11.400936Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Masked attention weights: tf.Tensor(\n[[[[ 6.2196958e-01 -1.0000000e+10 -1.0000000e+10 -1.0000000e+10\n    -1.0000000e+10]\n   [ 3.5770154e-01  2.3619473e-01 -1.0000000e+10 -1.0000000e+10\n    -1.0000000e+10]\n   [ 2.1580112e-01  3.4929395e-02  1.6679084e-01 -1.0000000e+10\n    -1.0000000e+10]\n   [ 4.9473345e-01  2.1166432e-01  3.2739556e-01  5.5245554e-01\n    -1.0000000e+10]\n   [ 5.0063765e-01  1.5066671e-01  6.9962549e-01  8.6920905e-01\n     8.9490736e-01]]]], shape=(1, 1, 5, 5), dtype=float32)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Scaled Dot-Product Attention</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"The scaled dot product attention is calculated as follows :\n\n***Attention(q, k, v) = softmax(qk^T / √(d_k))v***\n\nWhere **q**, **k** and **v** are the query, key, and value matrices and **d_k** is the dimensionality of the key matrix.\n\n[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-19.png\" width=\"100\">](https://arxiv.org/abs/1706.03762)","metadata":{}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, use_causal_mask=False):\n    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n    scores = tf.matmul(q, k, transpose_b=True) # Matmul of Q and K\n    scaled_scores = scores / tf.math.sqrt(d_k) # Scale\n    if use_causal_mask:\n        scaled_scores = mask_attn_weights(scaled_scores) # Mask (opt.)\n    weights = tf.nn.softmax(scaled_scores, axis=-1) # SoftMax\n    outputs = tf.matmul(weights, v) # Matmul of SoftMax and V\n    return outputs","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.403511Z","iopub.execute_input":"2023-03-10T20:34:11.403936Z","iopub.status.idle":"2023-03-10T20:34:11.410760Z","shell.execute_reply.started":"2023-03-10T20:34:11.403900Z","shell.execute_reply":"2023-03-10T20:34:11.409404Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# display the shape of our attention output just to test the function\ninput = train_source_embedded\ninput = tf.expand_dims(input, axis=1)\nprint(\"Scaled dot product attention (shape):\", scaled_dot_product_attention(input, input, input, use_causal_mask=True).shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:11.412309Z","iopub.execute_input":"2023-03-10T20:34:11.412927Z","iopub.status.idle":"2023-03-10T20:34:13.003133Z","shell.execute_reply.started":"2023-03-10T20:34:11.412891Z","shell.execute_reply":"2023-03-10T20:34:13.002029Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Scaled dot product attention (shape): (122934, 1, 30, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"color:#fff; display:fill; border-width:1px; border-color:#000; font-size:125%;\">\n    <p style=\"border-style:solid; border-radius:8px; background-color:#fff; padding: 8px 12px 8px 12px; color:#000;\"><b>Multi-Head Attention</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"The multi-head attention is calculated as follows :\n\n***MultiHead(q, k, v) = Concat(head_1, ..., head_h)W^O*** \n\nWhere head_i is the i-th attention head, W^O is the output matrix, and **h** is the number of attention heads.\n\n***head_i = Attention(qW_i^q, kW_i^k, vW_i^v)***\n\nWhere W_i^q, W_i^k, and W_i^v are the matrices for the i-th attention head.\n    \n[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-20.png\" width=\"200\">](https://arxiv.org/abs/1706.03762)","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, h, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.h = h\n        if embed_dim % h != 0:\n            raise ValueError(\n                f\"dimension of the embedding space = {embed_dim} should be divisible by number of heads = {h}\"\n            )\n        self.q_linear = tf.keras.layers.Dense(embed_dim)\n        self.k_linear = tf.keras.layers.Dense(embed_dim)\n        self.v_linear = tf.keras.layers.Dense(embed_dim)\n        self.concat_linear = tf.keras.layers.Dense(embed_dim)\n\n    def split_heads(self, x, batch_size):\n        x = tf.reshape(x, shape=(batch_size, -1, self.h, self.embed_dim // self.h))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n    \n    def concat_heads(self, x, batch_size):\n        x = tf.transpose(x, perm=[0, 2, 1, 3])\n        return tf.reshape(x, (batch_size, -1, self.embed_dim))\n\n    def call(self, q, k, v, use_causal_mask=False):\n        batch_size = tf.shape(k)[0]\n        q = self.q_linear(q)\n        k = self.k_linear(k)\n        v = self.v_linear(v)\n        q = self.split_heads(q, batch_size)\n        k = self.split_heads(k, batch_size)\n        v = self.split_heads(v, batch_size)\n        attention = scaled_dot_product_attention(q, k, v, use_causal_mask)\n        concat = self.concat_heads(attention, batch_size)\n        concat = self.concat_linear(concat)\n        return concat\n\n    def get_config(self):\n        config = super(MultiHeadAttention, self).get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"h\": self.h,\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:13.007282Z","iopub.execute_input":"2023-03-10T20:34:13.007780Z","iopub.status.idle":"2023-03-10T20:34:13.019871Z","shell.execute_reply.started":"2023-03-10T20:34:13.007750Z","shell.execute_reply":"2023-03-10T20:34:13.018613Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Encoder</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_encoder-zoom.png\" width=\"400\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n\nThe role of the Encoder is to process the source sequence. Here, no Causal Masking is needed : information is allowed to flow in both directions.\n\nThe Encoder's a pretty generic module that ingests a sequence and learns to turn it into a more useful representation. It can also be used by itself (without the Decoder) for Natural Language Understanding (NLU) tasks like classification or named entity recognition (NER).\n\nIn the Encoder's Multi-Head Self-Attention layer ([Global self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_global_self_attention_layer)), the **Source Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue.","metadata":{}},{"cell_type":"code","source":"class TransformerEncoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n        self.global_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n        self.feed_forward = tf.keras.Sequential(\n            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n             tf.keras.layers.Dense(embed_dim),]\n        )\n        \n    def call(self, x):\n        # Post layer normalization\n        x = self.layer_norm_1(x + self.global_self_attention(q=x, k=x, v=x))\n        x = self.layer_norm_2(x + self.feed_forward(x))\n        return x\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:13.021704Z","iopub.execute_input":"2023-03-10T20:34:13.022094Z","iopub.status.idle":"2023-03-10T20:34:13.032723Z","shell.execute_reply.started":"2023-03-10T20:34:13.022056Z","shell.execute_reply":"2023-03-10T20:34:13.031414Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>The Decoder</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_decoder-zoom.png\" width=\"500\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n\nThe role of the Decoder is to look at the target sequence so far and predict the next token in the sequence.\n\nContrary to the Encoder, the Decoder is made of two Attention layers. The first Attention layer does a similar job as the Encoder's sole Attention layer, with the important distinction that here, Causal Masking is enabled because, to correctly train our Transformer to predict the next token, we need to mask the *future* tokens. Meanwhile, The second Attention layer is much more straight-forward and basically just acts as a bridge that connects the Encoder to the Decoder.\n\nIn the Decoder's first Attention layer, the Masked Multi-Head Self-Attention layer ([Causal self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer)), the **Target Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue. Like mentionned above, Causal Masking is enabled in this layer.\n\nIn the Decoder's second Attention layer, the Encoder-Decoder Attention layer ([Cross attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_cross_attention_layer)), the **outputs of the Encoder** are being passed to the **K**ey and **V**alue parameters, with the **outputs of the Decoder's Masked Multi-Head Self-Attention layer** being passed to the **Q**uery parameter.\n","metadata":{}},{"cell_type":"code","source":"class TransformerDecoder(tf.keras.layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.causal_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n        self.cross_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n        self.feed_forward = tf.keras.Sequential(\n            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n             tf.keras.layers.Dense(embed_dim),]\n        )\n        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n        self.layer_norm_3 = tf.keras.layers.LayerNormalization()\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"embed_dim\": self.embed_dim,\n            \"dense_dim\": self.dense_dim,\n            \"num_heads\": self.num_heads,\n        })\n        return config\n\n    def call(self, x, context):\n        # Post layer normalization\n        x = self.layer_norm_1(x + self.causal_self_attention(q=x, k=x, v=x, use_causal_mask=True))\n        x = self.layer_norm_2(x + self.cross_attention(q=x, k=context, v=context))\n        x = self.layer_norm_3(x + self.feed_forward(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:13.034205Z","iopub.execute_input":"2023-03-10T20:34:13.035037Z","iopub.status.idle":"2023-03-10T20:34:13.047005Z","shell.execute_reply.started":"2023-03-10T20:34:13.034998Z","shell.execute_reply":"2023-03-10T20:34:13.046027Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"color:#000; display:fill; border-radius:8px; background-color:#000; font-size:125%;\">\n    <p style=\"padding: 8px 12px 8px 12px; color:#fff;\"><b>Putting it all together</b></p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n\nWe  turn our data into a *tf.data pipeline* that returns a tuple (Inputs, Outputs) where Inputs is a dict with two keys : **encoder_inputs** (the source sequence) and **decoder_inputs** (the target sequence), and Outputs is a single key : **decoder_outputs** (the target sentence \"shifted right\").\n\nDuring training, the fact that our Outputs are offset by one step ahead (\"shifted right\"), combined with the Causal Masking of the Decoder (Masked Multi-Head Attention layer), ensures that the\npredictions for position *i* can depend only on the known outputs at positions less than *i* (no *future* token is seen).\n\nDuring inference, we'll generate one target token at a time and then feed it back into the Decoder so that it can predict the next token. And so on.","metadata":{}},{"cell_type":"code","source":"batch_size = 64\n\ndef format_dataset(source, target):\n    source_vectors = source_vectorization(source)\n    target_vectors = target_vectorization(target)\n    return ({\n        \"source\": source_vectors, # encoder_inputs\n        \"target\": target_vectors[:, :-1], # decoder_inputs (truncate by 1 to keep it at the same length as decoder_outputs, which is shifted right by 1).\n    }, target_vectors[:, 1:]) # decoder_outputs\n\ndef make_dataset(df):\n    dataset = tf.data.Dataset.from_tensor_slices((df[\"source\"].values, df[\"target\"].values))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\ntrain_ds = make_dataset(train_df)\nval_ds = make_dataset(val_df)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:13.048637Z","iopub.execute_input":"2023-03-10T20:34:13.048967Z","iopub.status.idle":"2023-03-10T20:34:13.320084Z","shell.execute_reply.started":"2023-03-10T20:34:13.048927Z","shell.execute_reply":"2023-03-10T20:34:13.319046Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# display the shape of the first batch of data in the dataset just to see what it looks like\nfor batch in train_ds.take(1):\n    print(\"Encoder Inputs:\", batch[0][\"source\"].shape)\n    print(\"Decoder Inputs:\", batch[0][\"target\"].shape)\n    print(\"Decoder Outputs:\", batch[1].shape)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:13.321840Z","iopub.execute_input":"2023-03-10T20:34:13.322206Z","iopub.status.idle":"2023-03-10T20:34:14.086482Z","shell.execute_reply.started":"2023-03-10T20:34:13.322169Z","shell.execute_reply":"2023-03-10T20:34:14.085406Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Encoder Inputs: (64, 30)\nDecoder Inputs: (64, 30)\nDecoder Outputs: (64, 30)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_transformer-encoder-decoder.png\" width=\"600\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n","metadata":{}},{"cell_type":"code","source":"embed_dim = 512 # dimension of the embedding space\ndense_dim = 2048 # dimension of the feed forward network (a rule of thumb is to use 4 times the size of the embeddings)\nnum_heads = 8\n\n# the transformer body\nencoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\nx = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(encoder_inputs)\nencoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\ndecoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\nx = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(decoder_inputs)\nx = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n\n# the transformer head\nx = tf.keras.layers.Dropout(0.5)(x)\ndecoder_outputs = tf.keras.layers.Dense(max_tokens, activation=\"softmax\")(x)\n\ntransformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:14.087822Z","iopub.execute_input":"2023-03-10T20:34:14.088162Z","iopub.status.idle":"2023-03-10T20:34:15.527732Z","shell.execute_reply.started":"2023-03-10T20:34:14.088129Z","shell.execute_reply":"2023-03-10T20:34:15.526626Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Training the Transformer","metadata":{}},{"cell_type":"code","source":"transformer.compile(\n    optimizer=\"rmsprop\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"])\n\nEPOCHS = 50\ncheckpoint_filepath = '/tmp/checkpoint/'\ncallbacks_list = [\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_loss',\n        factor=0.1,\n        patience=3,\n    ),\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        patience=6,\n    ),\n    tf.keras.callbacks.ModelCheckpoint(\n        filepath=checkpoint_filepath,\n        save_weights_only=True,\n        monitor='val_loss',\n        mode='min',\n        save_best_only=True\n    ),\n]\n    \ntransformer.fit(train_ds, \n                epochs=EPOCHS, \n                callbacks=callbacks_list,\n                validation_data=val_ds)\n\ntransformer.load_weights(checkpoint_filepath)","metadata":{"execution":{"iopub.status.busy":"2023-03-10T20:34:15.529276Z","iopub.execute_input":"2023-03-10T20:34:15.529905Z","iopub.status.idle":"2023-03-10T21:39:20.048842Z","shell.execute_reply.started":"2023-03-10T20:34:15.529864Z","shell.execute_reply":"2023-03-10T21:39:20.047379Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Epoch 1/50\n1921/1921 [==============================] - 235s 117ms/step - loss: 1.0998 - accuracy: 0.8344 - val_loss: 0.6480 - val_accuracy: 0.8921 - lr: 0.0010\nEpoch 2/50\n1921/1921 [==============================] - 210s 109ms/step - loss: 0.6043 - accuracy: 0.8979 - val_loss: 0.5138 - val_accuracy: 0.9082 - lr: 0.0010\nEpoch 3/50\n1921/1921 [==============================] - 210s 109ms/step - loss: 0.4871 - accuracy: 0.9120 - val_loss: 0.4610 - val_accuracy: 0.9160 - lr: 0.0010\nEpoch 4/50\n1921/1921 [==============================] - 210s 109ms/step - loss: 0.4219 - accuracy: 0.9203 - val_loss: 0.4390 - val_accuracy: 0.9199 - lr: 0.0010\nEpoch 5/50\n1921/1921 [==============================] - 209s 109ms/step - loss: 0.3768 - accuracy: 0.9264 - val_loss: 0.4298 - val_accuracy: 0.9219 - lr: 0.0010\nEpoch 6/50\n1921/1921 [==============================] - 209s 109ms/step - loss: 0.3412 - accuracy: 0.9316 - val_loss: 0.4254 - val_accuracy: 0.9242 - lr: 0.0010\nEpoch 7/50\n1921/1921 [==============================] - 208s 108ms/step - loss: 0.3125 - accuracy: 0.9359 - val_loss: 0.4301 - val_accuracy: 0.9238 - lr: 0.0010\nEpoch 8/50\n1921/1921 [==============================] - 208s 108ms/step - loss: 0.2882 - accuracy: 0.9397 - val_loss: 0.4279 - val_accuracy: 0.9255 - lr: 0.0010\nEpoch 9/50\n1921/1921 [==============================] - 208s 108ms/step - loss: 0.2665 - accuracy: 0.9432 - val_loss: 0.4277 - val_accuracy: 0.9264 - lr: 0.0010\nEpoch 10/50\n1921/1921 [==============================] - 209s 109ms/step - loss: 0.1854 - accuracy: 0.9569 - val_loss: 0.3639 - val_accuracy: 0.9345 - lr: 1.0000e-04\nEpoch 11/50\n1921/1921 [==============================] - 209s 109ms/step - loss: 0.1564 - accuracy: 0.9622 - val_loss: 0.3598 - val_accuracy: 0.9351 - lr: 1.0000e-04\nEpoch 12/50\n1921/1921 [==============================] - 209s 109ms/step - loss: 0.1436 - accuracy: 0.9646 - val_loss: 0.3595 - val_accuracy: 0.9354 - lr: 1.0000e-04\nEpoch 13/50\n1921/1921 [==============================] - 214s 111ms/step - loss: 0.1342 - accuracy: 0.9664 - val_loss: 0.3604 - val_accuracy: 0.9357 - lr: 1.0000e-04\nEpoch 14/50\n1921/1921 [==============================] - 221s 115ms/step - loss: 0.1268 - accuracy: 0.9679 - val_loss: 0.3612 - val_accuracy: 0.9357 - lr: 1.0000e-04\nEpoch 15/50\n1921/1921 [==============================] - 221s 115ms/step - loss: 0.1209 - accuracy: 0.9691 - val_loss: 0.3624 - val_accuracy: 0.9357 - lr: 1.0000e-04\nEpoch 16/50\n1921/1921 [==============================] - 223s 116ms/step - loss: 0.1122 - accuracy: 0.9709 - val_loss: 0.3606 - val_accuracy: 0.9359 - lr: 1.0000e-05\nEpoch 17/50\n1921/1921 [==============================] - 229s 119ms/step - loss: 0.1104 - accuracy: 0.9713 - val_loss: 0.3602 - val_accuracy: 0.9360 - lr: 1.0000e-05\nEpoch 18/50\n1921/1921 [==============================] - 224s 116ms/step - loss: 0.1095 - accuracy: 0.9715 - val_loss: 0.3604 - val_accuracy: 0.9361 - lr: 1.0000e-05\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7fe8a812d390>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Testing the Transformer","metadata":{}},{"cell_type":"markdown","source":"Let's translate a few random test sentences with our newly-trained Transformer.","metadata":{}},{"cell_type":"code","source":"target_vocab = target_vectorization.get_vocabulary()\ntarget_index_lookup = dict(zip(range(len(target_vocab)), target_vocab))\nmax_decoded_sentence_length = 30\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = source_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = target_vectorization(\n            [decoded_sentence])[:, :-1]\n        predictions = transformer(\n            [tokenized_input_sentence, tokenized_target_sentence])\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = target_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n# let's translate 50 random sentences\nfor i in range(50):\n    random_index = np.random.randint(0, len(test_df))\n    input_sentence = test_df[\"source\"].iloc[random_index]\n    print(input_sentence)\n    print(decode_sequence(input_sentence))\n    print()","metadata":{"execution":{"iopub.status.busy":"2023-03-10T21:39:20.051007Z","iopub.execute_input":"2023-03-10T21:39:20.051564Z","iopub.status.idle":"2023-03-10T21:39:53.506563Z","shell.execute_reply.started":"2023-03-10T21:39:20.051511Z","shell.execute_reply":"2023-03-10T21:39:53.504615Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"I want a martini.\n[start] je veux un [UNK] [end]\n\nI'd like to introduce you to my wife.\n[start] jaimerais te présenter ma femme [end]\n\nHe is used to speaking in public.\n[start] il a lhabitude de parler en public [end]\n\nHe complained about the noise.\n[start] il sest plaint du bruit [end]\n\nI'm real careful.\n[start] je suis vraiment prudent [end]\n\nMy French is actually not so good.\n[start] mon français nest en réalité pas si bon [end]\n\nDon't think about the price.\n[start] ny pense pas le prix [end]\n\nHow long do they wish to spend here?\n[start] combien de temps ils veulent passer ici [end]\n\nDo you see anything?\n[start] estce que tu vois quelque chose [end]\n\nThe argument is rigorous and coherent but ultimately unconvincing.\n[start] le argument est en train et [UNK] mais un [UNK] [end]\n\nLet's put up the Christmas tree here.\n[start] mettons un sapin de noël ici [end]\n\nShe has two brothers, who work in the computer industry.\n[start] elle a deux frères qui travaille dans lindustrie informatique [end]\n\nI'll go first.\n[start] jirai en premier [end]\n\nWhat kind of meal did you eat?\n[start] quel genre de repas avezvous mangé [end]\n\nHe couldn't go out because of the snow.\n[start] il ne pouvait sortir à cause de la neige [end]\n\nI am married and I have two sons.\n[start] je suis marié et jai deux fils [end]\n\nDo you have a theory?\n[start] avezvous une théorie [end]\n\nIt's so easy.\n[start] cest tellement facile [end]\n\nEnjoy your day.\n[start] faites un jour [end]\n\nThe place where he lives is far from town.\n[start] lendroit où il vit à distance en ville [end]\n\nThe mail can't be delivered.\n[start] le courrier ne peut pas être distribué [end]\n\nThat's what everyone's saying.\n[start] cest ce que tout le monde dit [end]\n\nI read all kinds of books.\n[start] jai tout lu sortes de livres [end]\n\nAre all of them your friends?\n[start] tous vos amis [end]\n\nThe party was held on May 22nd.\n[start] la fête sest tenue à un [UNK] [end]\n\nI'm afraid we have no choice.\n[start] je crains que nous nayons non [end]\n\nMy job is to anticipate problems.\n[start] mon travail est de [UNK] aux problèmes [end]\n\nDon't be so outraged.\n[start] ne soyez pas si indignée [end]\n\nYou should eat.\n[start] tu devrais manger [end]\n\nAre you tall?\n[start] Êtesvous grande [end]\n\nTom fell down the stairs.\n[start] tom est tombé dans les escaliers [end]\n\nDon't dwell on your past mistakes!\n[start] ne ressasse pas tes erreurs passées [end]\n\nYou're a horrible driver.\n[start] vous avez une horrible conducteur [end]\n\nI don't think you would do that.\n[start] je ne pense pas que tu ferais ça [end]\n\nI am going to buy a new car.\n[start] je vais acheter une nouvelle voiture [end]\n\nShe is used to cooking.\n[start] elle est habituée à cuisiner [end]\n\nHe likes vegetables, especially cabbage.\n[start] il aime particulièrement les légumes particulièrement [end]\n\nI had a very high fever.\n[start] jai eu une très haut de fièvre [end]\n\nHe has enough ability to manage a business.\n[start] il a suffisamment capacités pour faire un travail [end]\n\nHe laid down his pen and leaned back in his chair.\n[start] il [UNK] son stylo et dans le dos [end]\n\nIt's more complicated than I originally thought.\n[start] cest plus compliqué que moi je pensais [end]\n\nI don't know what I'm doing here.\n[start] je ne sais pas ce que je fais ici [end]\n\nThat movie was pretty boring.\n[start] ce film était assez ennuyeux [end]\n\nHe can't come with us.\n[start] il ne peut pas venir avec nous [end]\n\nI've found a new job.\n[start] jai trouvé un nouveau travail [end]\n\nI'm not presentable.\n[start] je ne suis pas au courant [end]\n\nThat baby is fat and healthy.\n[start] ce bébé est gros et en bonne santé [end]\n\nHe doesn't have his feet on the ground.\n[start] il ne dispose pas de ses pieds sur le sol [end]\n\nI did that voluntarily.\n[start] jai fait ça volontairement [end]\n\nMy short-term memory is getting shorter and shorter.\n[start] mon [UNK] est de la mémoire court et plus court [end]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Credits and stuff\n\n- https://arxiv.org/abs/1706.03762\n\n- https://www.tensorflow.org/text/tutorials/transformer\n\n- https://github.com/openai/gpt-2/blob/master/src/model.py\n\n- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n\n- https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11\n\n    - https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n\n    - https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n\n- https://www.oreilly.com/library/view/natural-language-processing/9781098136789/ch03.html\n\n    - https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb","metadata":{}}]}