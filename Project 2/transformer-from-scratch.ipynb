{"cells":[{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2023-03-07T21:56:23.313623Z","iopub.status.busy":"2023-03-07T21:56:23.313156Z","iopub.status.idle":"2023-03-07T21:56:23.320534Z","shell.execute_reply":"2023-03-07T21:56:23.318865Z","shell.execute_reply.started":"2023-03-07T21:56:23.313584Z"}},"source":["# Transformer from scratch\n","\n","This notebook builds a simple Transformer from scratch (Multi-Head Attention, Scaled Dot-Product Attention and Causal Masking included) in tensorflow.\n","\n","It serves to demonstrate how each part of the Transformer works and how they all fit together.\n","\n","The Transformer is then tested on a simple seq2seq task: translating sentences from English to French.\n","\n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n","\n","**Steps :**\n","1. [Preparing the data](#Preparing-the-data)\n","2. [Building the Transformer](#Building-the-Transformer)\n","3. [Training the Transformer](#Training-the-Transformer)\n","4. [Testing the Transformer](#Testing-the-Transformer)\n","    \n","*[Credits and stuff](#Credits-and-stuff)*"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:23:52.030266Z","iopub.status.busy":"2023-03-08T19:23:52.029669Z","iopub.status.idle":"2023-03-08T19:23:54.904426Z","shell.execute_reply":"2023-03-08T19:23:54.903327Z","shell.execute_reply.started":"2023-03-08T19:23:52.030218Z"},"trusted":true},"outputs":[],"source":["import os\n","import random\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import string\n","import re\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:23:54.911509Z","iopub.status.busy":"2023-03-08T19:23:54.911146Z","iopub.status.idle":"2023-03-08T19:23:54.998087Z","shell.execute_reply":"2023-03-08T19:23:54.996954Z","shell.execute_reply.started":"2023-03-08T19:23:54.911470Z"},"trusted":true},"outputs":[],"source":["print(f\"Tensor Flow Version: {tf.__version__}\")\n","print()\n","gpu = len(tf.config.list_physical_devices('GPU'))>0\n","print(\"GPU is\", \"AVAILABLE\" if gpu else \"NOT AVAILABLE\")"]},{"cell_type":"markdown","metadata":{},"source":["# Preparing the data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Parsing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:23:55.000940Z","iopub.status.busy":"2023-03-08T19:23:55.000050Z","iopub.status.idle":"2023-03-08T19:23:55.228446Z","shell.execute_reply":"2023-03-08T19:23:55.226437Z","shell.execute_reply.started":"2023-03-08T19:23:55.000897Z"},"trusted":true},"outputs":[],"source":["# parse the file\n","path = \"/kaggle/input\"\n","path = path if os.path.exists(path) else \".{}\".format(path)\n","\n","text_file = os.path.join(path, \"fra-eng\", \"fra.txt\")\n","with open(text_file) as f:\n","    lines = f.read().split(\"\\n\")[:-1]\n","data = []\n","for line in lines:\n","    english, french = line.split(\"\\t\")\n","    french = \"[start] \" + french + \" [end]\"\n","    data.append((english, french))\n","\n","# display a few random samples just to get an idea of the data\n","for i in range(5):\n","    print(data[random.randint(0, len(data))])"]},{"cell_type":"markdown","metadata":{},"source":["## Shuffling the data and splitting it into train, validation, and test sets:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:23:55.232476Z","iopub.status.busy":"2023-03-08T19:23:55.232169Z","iopub.status.idle":"2023-03-08T19:23:55.567568Z","shell.execute_reply":"2023-03-08T19:23:55.565846Z","shell.execute_reply.started":"2023-03-08T19:23:55.232445Z"},"trusted":true},"outputs":[],"source":["# shuffle the data\n","random.shuffle(data)\n","\n","# split the data into train, validation, and test sets\n","train = data[:int(len(data)*0.7)]\n","validation = data[int(len(data)*0.7):int(len(data)*0.9)]\n","test = data[int(len(data)*0.9):]\n","\n","# display the data sets representations using a pie chart just to see the distribution of the data\n","labels = 'Train', 'Validation', 'Test'\n","sizes = [len(train), len(validation), len(test)]\n","explode = (0.1, 0, 0)\n","fig1, ax1 = plt.subplots()\n","ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n","ax1.axis('equal')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Standardizing, tokenizing, and indexing the data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This step takes our raw text data and vectorizes it.\n","\n","To keep things simple, we will first limit our vocabulary using the **max_tokens** parameter. We will also limit the length of each sample using the **sequence_length** parameter.\n","\n","Each sample (text input) will be **standardize**, **tokenize** by word, and then **indexed** by token.\n","\n","This will result in a matrix of vectors (input IDs) of shape \"(batch_size, **sequence_length**)\"."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:23:55.570045Z","iopub.status.busy":"2023-03-08T19:23:55.569576Z","iopub.status.idle":"2023-03-08T19:24:19.079470Z","shell.execute_reply":"2023-03-08T19:24:19.077845Z","shell.execute_reply.started":"2023-03-08T19:23:55.570001Z"},"trusted":true},"outputs":[],"source":["max_tokens = 20000\n","sequence_length = 30\n","\n","# define a custom standardization function that convert to lowercase and strip all punctuations except \"[\" and \"]\", \n","# so we can tell apart the words \"start\" and \"end\" from the tokens \"[start]\" and \"[end]\"\n","strip_chars = string.punctuation\n","strip_chars = strip_chars.replace(\"[\", \"\")\n","strip_chars = strip_chars.replace(\"]\", \"\")\n"," \n","def custom_standardization(input_string):\n","    lowercase = tf.strings.lower(input_string)\n","    return tf.strings.regex_replace(\n","        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n","\n","# tokenize the data using our custom standardization function\n","source_vectorization = tf.keras.layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length,\n",")\n","target_vectorization = tf.keras.layers.TextVectorization(\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",\n","    output_sequence_length=sequence_length + 1, # add +1 token to our target sentences since they'll be shifted right by 1 during training\n","    standardize=custom_standardization,\n",")\n","\n","# index all tokens with the adapt() method\n","train_source_texts = [text[0] for text in train]\n","train_target_texts = [text[1] for text in train]\n","source_vectorization.adapt(train_source_texts)\n","target_vectorization.adapt(train_target_texts)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:19.081862Z","iopub.status.busy":"2023-03-08T19:24:19.081085Z","iopub.status.idle":"2023-03-08T19:24:21.962102Z","shell.execute_reply":"2023-03-08T19:24:21.960950Z","shell.execute_reply.started":"2023-03-08T19:24:19.081820Z"},"trusted":true},"outputs":[],"source":["# display a random sample before and after vectorization just to test the vectorization\n","random_sample = random.randint(0, len(train))\n","print(\"Source texts (one random sample):\", train_source_texts[random_sample])\n","print(\"Target texts (one random sample):\", train_target_texts[random_sample])\n","print(\"Source vectors (one random sample):\", source_vectorization(train_source_texts[random_sample]))\n","print(\"Target vectors (one random sample):\", target_vectorization(train_target_texts[random_sample]))\n","\n","# display the decoding of the vectorized text (from vector back to text) just to test the vectorization\n","source_decoded_text = ''\n","for i in range(len(source_vectorization(train_source_texts[random_sample]))):\n","    source_decoded_text += source_vectorization.get_vocabulary()[source_vectorization(train_source_texts[random_sample])[i]] + ' '\n","print(\"Source decoded texts (one random sample):\", source_decoded_text)\n","\n","target_decoded_text = ''\n","for i in range(len(target_vectorization(train_target_texts[random_sample]))):\n","    target_decoded_text += target_vectorization.get_vocabulary()[target_vectorization(train_target_texts[random_sample])[i]] + ' '\n","print(\"Target decoded texts (one random sample):\", target_decoded_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:21.964195Z","iopub.status.busy":"2023-03-08T19:24:21.963804Z","iopub.status.idle":"2023-03-08T19:24:23.474873Z","shell.execute_reply":"2023-03-08T19:24:23.473544Z","shell.execute_reply.started":"2023-03-08T19:24:21.964156Z"},"trusted":true},"outputs":[],"source":["# display the shape of our vectorized data\n","train_source_vectors = source_vectorization(train_source_texts)\n","train_target_vectors = target_vectorization(train_target_texts)\n","print(\"Source vectors (shape):\", train_source_vectors.shape)\n","print(\"Target vectors (shape):\", train_target_vectors.shape)"]},{"cell_type":"markdown","metadata":{},"source":["# Building the Transformer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Positional Embedding"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This step takes our previously tokenized data (input IDs) of shape \"(batch_size, sequence_length)\" and embeds it while adding positional information.\n","\n","\n","Each vector of tokens will first be **embedded** in a low-dimensional floating-point vectors (the dimensionality of the embedding space is defined by the **embedding_size** parameter).\n","\n","**Position embeddings** will then created and added to those embedded tokens.\n","\n","This will result in positional embeddings (position-aware word embeddings) stored in a matrix of shape \"(batch_size, sequence_length, **embedding_size**)\"."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:23.477802Z","iopub.status.busy":"2023-03-08T19:24:23.476895Z","iopub.status.idle":"2023-03-08T19:24:23.487206Z","shell.execute_reply":"2023-03-08T19:24:23.486150Z","shell.execute_reply.started":"2023-03-08T19:24:23.477737Z"},"trusted":true},"outputs":[],"source":["class PositionalEmbedding(tf.keras.layers.Layer):\n","    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim) # token embedding layer\n","        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=output_dim) # position embedding layer\n","        self.sequence_length = sequence_length\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def call(self, inputs):\n","        embedded_tokens = self.token_embeddings(inputs) # embed the tokens\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1) # create the positional information\n","        embedded_positions = self.position_embeddings(positions) # embed the positions \n","        return embedded_tokens + embedded_positions # add the token and position embeddings to create the positional embeddings\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super(PositionalEmbedding, self).get_config()\n","        config.update({\n","            \"input_dim\": self.input_dim,\n","            \"output_dim\": self.output_dim,\n","            \"sequence_length\": self.sequence_length,\n","        })\n","        return config"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:23.489272Z","iopub.status.busy":"2023-03-08T19:24:23.488814Z","iopub.status.idle":"2023-03-08T19:24:23.601569Z","shell.execute_reply":"2023-03-08T19:24:23.599668Z","shell.execute_reply.started":"2023-03-08T19:24:23.489230Z"},"trusted":true},"outputs":[],"source":["# display the shape of our embedded data just to test the class\n","\n","embed_dim = 256\n","\n","train_source_embedded = PositionalEmbedding(\n","    sequence_length=sequence_length,\n","    input_dim=max_tokens,\n","    output_dim=embed_dim,\n","    name=\"source_embedding\",\n",") (train_source_vectors)\n","\n","train_target_embedded = PositionalEmbedding(\n","    sequence_length=sequence_length,\n","    input_dim=max_tokens,\n","    output_dim=embed_dim,\n","    name=\"target_embedding\",\n",") (train_source_vectors)\n","\n","print(\"Source embedded vectors (shape):\", train_source_embedded.shape)\n","print(\"Target embedded vectors (shape):\", train_target_embedded.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## The Attention mechanism"]},{"cell_type":"markdown","metadata":{},"source":["A simple implementation of a Transformer's attention mechanism from scratch.\n","\n","In practice, we could just use **tf.keras.layers.MultiHeadAttention** instead of building it from scratch. 😒"]},{"cell_type":"markdown","metadata":{},"source":["### Scaled Dot-Product Attention"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The scaled dot product attention is calculated as follows :\n","\n","***Attention(q, k, v) = softmax(qk^T / √(d_k))v***\n","\n","Where **q**, **k** and **v** are the query, key, and value matrices and **d_k** is the dimensionality of the key matrix.\n","\n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-19.png\" width=\"100\">](https://arxiv.org/abs/1706.03762)\n","\n","We'll also add optional causal masking (**use_causal_mask**) to allow masking of \"future\" data. This will be used in the decoder's implementation."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:23.603502Z","iopub.status.busy":"2023-03-08T19:24:23.603092Z","iopub.status.idle":"2023-03-08T19:24:23.616082Z","shell.execute_reply":"2023-03-08T19:24:23.615011Z","shell.execute_reply.started":"2023-03-08T19:24:23.603461Z"},"trusted":true},"outputs":[],"source":["def shape_list(x):\n","    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n","    static = x.shape.as_list()\n","    dynamic = tf.shape(x)\n","    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n","\n","def attention_mask(nd, ns, *, dtype):\n","    \"\"\"1's in the lower triangle, counting from the lower right corner.\n","    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n","    \"\"\"\n","    i = tf.range(nd)[:,None]\n","    j = tf.range(ns)\n","    m = i >= j - ns + nd\n","    return tf.cast(m, dtype)\n","\n","def mask_attn_weights(w):\n","    # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n","    _, _, nd, ns = shape_list(w)\n","    b = attention_mask(nd, ns, dtype=w.dtype)\n","    b = tf.reshape(b, [1, 1, nd, ns])\n","    w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n","    return w\n","    \n","def scaled_dot_product_attention(q, k, v, use_causal_mask=False):\n","    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n","    scores = tf.matmul(q, k, transpose_b=True) # Matmul of Q and K\n","    scaled_scores = scores / tf.math.sqrt(d_k) # Scale\n","    if use_causal_mask:\n","        scaled_scores = mask_attn_weights(scaled_scores) # Mask (opt.)\n","    weights = tf.nn.softmax(scaled_scores, axis=-1) # SoftMax\n","    outputs = tf.matmul(weights, v) # Matmul of SoftMax and V\n","    return outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:23.618698Z","iopub.status.busy":"2023-03-08T19:24:23.617468Z","iopub.status.idle":"2023-03-08T19:24:24.101127Z","shell.execute_reply":"2023-03-08T19:24:24.099956Z","shell.execute_reply.started":"2023-03-08T19:24:23.618655Z"},"trusted":true},"outputs":[],"source":["# display the shape of our attention output just to test the function\n","input = train_source_embedded\n","input = tf.expand_dims(input, axis=1)\n","print(\"Scaled dot product attention (shape):\", scaled_dot_product_attention(input, input, input, use_causal_mask=True).shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:24.103357Z","iopub.status.busy":"2023-03-08T19:24:24.102601Z","iopub.status.idle":"2023-03-08T19:24:24.190509Z","shell.execute_reply":"2023-03-08T19:24:24.189442Z","shell.execute_reply.started":"2023-03-08T19:24:24.103312Z"},"trusted":true},"outputs":[],"source":["# display the causal masking of a random tensor just to test the function\n","random_tensor = tf.random.uniform(shape=(1, 1, 5, 5), minval=0, maxval=1, dtype=tf.float32)\n","print(\"Masked attention weights:\", mask_attn_weights(random_tensor))"]},{"cell_type":"markdown","metadata":{},"source":["### Multi-Head Attention"]},{"cell_type":"markdown","metadata":{},"source":["The multi-head attention is calculated as follows :\n","\n","***MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O***\n","\n","Where head_i is the i-th attention head, W^O is the output matrix, and **h** is the number of attention heads.\n","    \n","[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-20.png\" width=\"200\">](https://arxiv.org/abs/1706.03762)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:24.195494Z","iopub.status.busy":"2023-03-08T19:24:24.195209Z","iopub.status.idle":"2023-03-08T19:24:24.209205Z","shell.execute_reply":"2023-03-08T19:24:24.207553Z","shell.execute_reply.started":"2023-03-08T19:24:24.195467Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, h, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.h = h\n","        if embed_dim % h != 0:\n","            raise ValueError(\n","                f\"dimension of the embedding space = {embed_dim} should be divisible by number of heads = {h}\"\n","            )\n","        self.q_linear = tf.keras.layers.Dense(embed_dim)\n","        self.k_linear = tf.keras.layers.Dense(embed_dim)\n","        self.v_linear = tf.keras.layers.Dense(embed_dim)\n","        self.concat_linear = tf.keras.layers.Dense(embed_dim)\n","\n","    def split_heads(self, x, batch_size):\n","        x = tf.reshape(x, shape=(batch_size, -1, self.h, self.embed_dim // self.h))\n","        return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","    def concat_heads(self, x, batch_size):\n","        x = tf.transpose(x, perm=[0, 2, 1, 3])\n","        return tf.reshape(x, (batch_size, -1, self.embed_dim))\n","\n","    def call(self, q, k, v, use_causal_mask=False):\n","        batch_size = tf.shape(k)[0]\n","        q = self.q_linear(q)\n","        k = self.k_linear(k)\n","        v = self.v_linear(v)\n","        q = self.split_heads(q, batch_size)\n","        k = self.split_heads(k, batch_size)\n","        v = self.split_heads(v, batch_size)\n","        attention = scaled_dot_product_attention(q, k, v, use_causal_mask)\n","        concat = self.concat_heads(attention, batch_size)\n","        concat = self.concat_linear(concat)\n","        return concat\n","\n","    def get_config(self):\n","        config = super(MultiHeadAttention, self).get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"h\": self.h,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["## The Encoder"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_encoder-zoom.png\" width=\"400\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n","\n","\n","In the Encoder's Multi-Head Self-Attention layer ([Global self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_global_self_attention_layer)), the **Source Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:24.211345Z","iopub.status.busy":"2023-03-08T19:24:24.210842Z","iopub.status.idle":"2023-03-08T19:24:24.224239Z","shell.execute_reply":"2023-03-08T19:24:24.223076Z","shell.execute_reply.started":"2023-03-08T19:24:24.211300Z"},"trusted":true},"outputs":[],"source":["class TransformerEncoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n","        self.global_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.feed_forward = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n","             tf.keras.layers.Dense(embed_dim),]\n","        )\n","        \n","    def call(self, x):\n","        # Post layer normalization\n","        x = self.layer_norm_1(x + self.global_self_attention(q=x, k=x, v=x))\n","        x = self.layer_norm_2(x + self.feed_forward(x))\n","        return x\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config"]},{"cell_type":"markdown","metadata":{},"source":["## The Decoder"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_decoder-zoom.png\" width=\"500\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n","\n","\n","In the Decoder's Masked Multi-Head Self-Attention layer ([Causal self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer)), the **Target Vectors Embeddings** are being passed to all three parameters : **Q**uery, **K**ey, and **V**alue.\n","\n","In the Decoder’s Encoder-Decoder Attention layer ([Cross attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_cross_attention_layer)), the **outputs of the Encoder** are being passed to the **K**ey and **V**alue parameters, with the **outputs of the Decoder's Masked Multi-Head Self-Attention layer** being passed to the **Q**uery parameter. Also, causal masking is used in this layer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:24.226623Z","iopub.status.busy":"2023-03-08T19:24:24.225844Z","iopub.status.idle":"2023-03-08T19:24:24.242117Z","shell.execute_reply":"2023-03-08T19:24:24.240731Z","shell.execute_reply.started":"2023-03-08T19:24:24.226540Z"},"trusted":true},"outputs":[],"source":["class TransformerDecoder(tf.keras.layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.causal_self_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.cross_attention = MultiHeadAttention(embed_dim=embed_dim, h=num_heads)\n","        self.feed_forward = tf.keras.Sequential(\n","            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n","             tf.keras.layers.Dense(embed_dim),]\n","        )\n","        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n","        self.layer_norm_3 = tf.keras.layers.LayerNormalization()\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update({\n","            \"embed_dim\": self.embed_dim,\n","            \"dense_dim\": self.dense_dim,\n","            \"num_heads\": self.num_heads,\n","        })\n","        return config\n","\n","    def call(self, x, context):\n","        # Post layer normalization\n","        x = self.layer_norm_1(x + self.causal_self_attention(q=x, k=x, v=x, use_causal_mask=True))\n","        x = self.layer_norm_2(x + self.cross_attention(q=x, k=context, v=context))\n","        x = self.layer_norm_3(x + self.feed_forward(x))\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["## Putting it all together"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">](https://arxiv.org/abs/1706.03762)\n","\n","We  turn our data into a tf.data pipeline. We want it to return a tuple (Inputs, Outputs) where Inputs is a dict with two keys, “encoder_inputs” (the source sequence) and “decoder_inputs” (the target sequence), and Outputs is a single key (the target sentence \"shifted right\").\n","\n","The role of the decoder is to look at the target sequence so far and predict the next token in the sequence.\n","\n","During training, the fact that our Outputs are offset by one step ahead (\"shifted right\"), combined with the causal masking, ensures that the\n","predictions for position *i* can depend only on the known outputs at positions less than *i*.\n","\n","During inference, we'll generate one target token at a time and then feed it back into the decoder so it can predict the next token. And so on."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:24.244399Z","iopub.status.busy":"2023-03-08T19:24:24.243941Z","iopub.status.idle":"2023-03-08T19:24:26.117121Z","shell.execute_reply":"2023-03-08T19:24:26.116074Z","shell.execute_reply.started":"2023-03-08T19:24:24.244358Z"},"trusted":true},"outputs":[],"source":["batch_size = 64\n","\n","def format_dataset(source, target):\n","    source_vectors = source_vectorization(source)\n","    target_vectors = target_vectorization(target)\n","    return ({\n","        \"source\": source_vectors, # encoder_inputs\n","        \"target\": target_vectors[:, :-1], # decoder_inputs (truncate by 1 to keep it at the same length as decoder_outputs, which is shifted right by 1).\n","    }, target_vectors[:, 1:]) # decoder_outputs\n","\n","def make_dataset(texts):\n","    source_texts, target_texts = zip(*texts)\n","    source_texts = list(source_texts)\n","    target_texts = list(target_texts)\n","    dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n","    return dataset.shuffle(2048).prefetch(16).cache()\n","\n","train_ds = make_dataset(train)\n","validation_ds = make_dataset(validation)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:26.119218Z","iopub.status.busy":"2023-03-08T19:24:26.118776Z","iopub.status.idle":"2023-03-08T19:24:26.823057Z","shell.execute_reply":"2023-03-08T19:24:26.821972Z","shell.execute_reply.started":"2023-03-08T19:24:26.119129Z"},"trusted":true},"outputs":[],"source":["#display the shape of the first batch of data in the dataset just to see what it looks like\n","for batch in train_ds.take(1):\n","    print(\"Encoder Inputs:\", batch[0][\"source\"].shape)\n","    print(\"Decoder Inputs:\", batch[0][\"target\"].shape)\n","    print(\"Decoder Outputs:\", batch[1].shape)"]},{"cell_type":"markdown","metadata":{},"source":["[<img src=\"https://raw.githubusercontent.com/nlp-with-transformers/notebooks/main/images/chapter03_transformer-encoder-decoder.png\" width=\"600\">](https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:26.825352Z","iopub.status.busy":"2023-03-08T19:24:26.824955Z","iopub.status.idle":"2023-03-08T19:24:27.977197Z","shell.execute_reply":"2023-03-08T19:24:27.976138Z","shell.execute_reply.started":"2023-03-08T19:24:26.825312Z"},"trusted":true},"outputs":[],"source":["embed_dim = 512 # dimension of the embedding space\n","dense_dim = 2048 # dimension of the feed forward network (a rule of thumb is to use 4 times the size of the embeddings)\n","num_heads = 8\n","\n","# transformer body\n","encoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n","x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n","decoder_inputs = tf.keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\n","x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n","\n","# transformer head\n","x = tf.keras.layers.Dropout(0.5)(x)\n","decoder_outputs = tf.keras.layers.Dense(max_tokens, activation=\"softmax\")(x)\n","\n","transformer = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"markdown","metadata":{},"source":["# Training the Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T19:24:27.979075Z","iopub.status.busy":"2023-03-08T19:24:27.978669Z","iopub.status.idle":"2023-03-08T20:21:09.534820Z","shell.execute_reply":"2023-03-08T20:21:09.533821Z","shell.execute_reply.started":"2023-03-08T19:24:27.979035Z"},"trusted":true},"outputs":[],"source":["transformer.compile(\n","    optimizer=\"rmsprop\",\n","    loss=\"sparse_categorical_crossentropy\",\n","    metrics=[\"accuracy\"])\n","\n","EPOCHS = 50\n","checkpoint_filepath = '/tmp/checkpoint/'\n","callbacks_list = [\n","    tf.keras.callbacks.ReduceLROnPlateau(\n","        monitor='val_loss',\n","        factor=0.1,\n","        patience=3,\n","    ),\n","    tf.keras.callbacks.EarlyStopping(\n","        monitor='val_loss',\n","        patience=6,\n","    ),\n","    tf.keras.callbacks.ModelCheckpoint(\n","        filepath=checkpoint_filepath,\n","        save_weights_only=True,\n","        monitor='val_loss',\n","        mode='min',\n","        save_best_only=True\n","    ),\n","]\n","    \n","transformer.fit(train_ds, \n","                epochs=EPOCHS, \n","                callbacks=callbacks_list,\n","                validation_data=validation_ds)\n","\n","transformer.load_weights(checkpoint_filepath)"]},{"cell_type":"markdown","metadata":{},"source":["# Testing the Transformer"]},{"cell_type":"markdown","metadata":{},"source":["Let's translate a few random test sentences with our trained Transformer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-03-08T20:21:09.536971Z","iopub.status.busy":"2023-03-08T20:21:09.536573Z","iopub.status.idle":"2023-03-08T20:21:31.040562Z","shell.execute_reply":"2023-03-08T20:21:31.039336Z","shell.execute_reply.started":"2023-03-08T20:21:09.536923Z"},"trusted":true},"outputs":[],"source":["target_vocab = target_vectorization.get_vocabulary()\n","target_index_lookup = dict(zip(range(len(target_vocab)), target_vocab))\n","max_decoded_sentence_length = 30\n","\n","def decode_sequence(input_sentence):\n","    tokenized_input_sentence = source_vectorization([input_sentence])\n","    decoded_sentence = \"[start]\"\n","    for i in range(max_decoded_sentence_length):\n","        tokenized_target_sentence = target_vectorization(\n","            [decoded_sentence])[:, :-1]\n","        predictions = transformer(\n","            [tokenized_input_sentence, tokenized_target_sentence])\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        sampled_token = target_index_lookup[sampled_token_index]\n","        decoded_sentence += \" \" + sampled_token\n","        if sampled_token == \"[end]\":\n","            break\n","    return decoded_sentence\n","\n","test_source_texts = [text[0] for text in test]\n","\n","# let's translate 50 random sentences\n","for _ in range(50):\n","    input_sentence = random.choice(test_source_texts)\n","    print(\"-\")\n","    print(input_sentence)\n","    print(decode_sequence(input_sentence))"]},{"cell_type":"markdown","metadata":{},"source":["# Credits and stuff\n","\n","- https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11\n","\n","- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n","\n","- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n","\n","- https://www.oreilly.com/library/view/natural-language-processing/9781098136789/ch03.html\n","\n","- https://github.com/nlp-with-transformers/notebooks/blob/main/03_transformer-anatomy.ipynb\n","\n","- https://arxiv.org/abs/1706.03762\n","\n","- https://www.tensorflow.org/text/tutorials/transformer\n","\n","- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n","\n","- https://github.com/openai/gpt-2/blob/master/src/model.py\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"1c472d3f56e1d3981a3fd37d3eee44d1ccdd21bd4124bc6c2c96c1c4dcfe0833"}}},"nbformat":4,"nbformat_minor":4}
