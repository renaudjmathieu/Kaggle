{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print()\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"AVAILABLE\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the file\n",
    "path = \"/kaggle/input\"\n",
    "path = path if os.path.exists(path) else \".{}\".format(path)\n",
    "\n",
    "text_file = os.path.join(path, \"fra-eng\", \"fra.txt\")\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "data = []\n",
    "for line in lines:\n",
    "    english, french = line.split(\"\\t\")\n",
    "    french = \"[start] \" + french + \" [end]\"\n",
    "    data.append((english, french))\n",
    "\n",
    "# display a few random samples just to get an idea of the data\n",
    "for i in range(5):\n",
    "    print(data[random.randint(0, len(data))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling the data and splitting it into train, validation, and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the data\n",
    "random.shuffle(data)\n",
    "\n",
    "# split the data into train, validation, and test sets\n",
    "train = data[:int(len(data)*0.7)]\n",
    "validation = data[int(len(data)*0.7):int(len(data)*0.9)]\n",
    "test = data[int(len(data)*0.9):]\n",
    "\n",
    "# display the data sets representations using a pie chart just to see the distribution of the data\n",
    "labels = 'Train', 'Validation', 'Test'\n",
    "sizes = [len(train), len(validation), len(test)]\n",
    "explode = (0.1, 0, 0)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "ax1.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes our raw text data and converts it in a format that can be used by our model.\n",
    "\n",
    "To keep things simple, we will first limit our vocabulary using the **max_tokens** parameter. We will also limit the length of each sample using the **sequence_length** parameter.\n",
    "\n",
    "Each sample (text input) will be standardize, tokenize by word, and then indexed by token.\n",
    "\n",
    "This will result in a matrix of vectors (input IDs) of shape (batch_size, **sequence_length**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "# convert to lowercase and strip all punctuations except \"[\" and \"]\", so we can tell apart the words \"start\" and \"end\" from the tokens \"[start]\" and \"[end]\"\n",
    "strip_chars = string.punctuation\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    " \n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    " \n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1, # we generate our target sentences that have one extra token, since weâ€™ll need to offset the sentence by one step during training\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_source_texts = [text[0] for text in train]\n",
    "train_target_texts = [text[1] for text in train]\n",
    "source_vectorization.adapt(train_source_texts)\n",
    "target_vectorization.adapt(train_target_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display a random sample before and after vectorization just to test the vectorization\n",
    "random_sample = random.randint(0, len(train))\n",
    "print(\"Source texts (one random sample):\", train_source_texts[random_sample])\n",
    "print(\"Target texts (one random sample):\", train_target_texts[random_sample])\n",
    "print(\"Source vectors (one random sample):\", source_vectorization(train_source_texts[random_sample]))\n",
    "print(\"Target vectors (one random sample):\", target_vectorization(train_target_texts[random_sample]))\n",
    "\n",
    "# display the decoding of the vectorized text (from vector back to text) just to test the vectorization\n",
    "source_decoded_text = ''\n",
    "for i in range(len(source_vectorization(train_source_texts[random_sample]))):\n",
    "    source_decoded_text += source_vectorization.get_vocabulary()[source_vectorization(train_source_texts[random_sample])[i]] + ' '\n",
    "print(\"Source decoded texts (one random sample):\", source_decoded_text)\n",
    "\n",
    "target_decoded_text = ''\n",
    "for i in range(len(target_vectorization(train_target_texts[random_sample]))):\n",
    "    target_decoded_text += target_vectorization.get_vocabulary()[target_vectorization(train_target_texts[random_sample])[i]] + ' '\n",
    "print(\"Target decoded texts (one random sample):\", target_decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the shape of our vectorized data\n",
    "train_source_vectors = source_vectorization(train_source_texts)\n",
    "train_target_vectors = target_vectorization(train_target_texts)\n",
    "print(\"Source vectors (shape):\", train_source_vectors.shape)\n",
    "print(\"Target vectors (shape):\", train_target_vectors.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding and Position Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step takes our matrix of vectors (input IDs) of shape (batch_size, sequence_length) and embeds it while adding positional information.\n",
    "\n",
    "Each vector will be embedded in a low-dimensional floating-point vectors (the dimensionality of the embedding space is defined by the **embedding_size** parameter).\n",
    "\n",
    "Each embedding vector will then be augmented with positional information.\n",
    "\n",
    "This will result in a matrix of shape (batch_size, sequence_length, **embedding_size**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = tf.keras.layers.Embedding(input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the shape of our embedded data just to test the class\n",
    "\n",
    "embed_dim = 256\n",
    "\n",
    "train_source_embedded = PositionalEmbedding(\n",
    "    sequence_length=sequence_length,\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=embed_dim,\n",
    "    name=\"source_embedding\",\n",
    ") (train_source_vectors)\n",
    "\n",
    "train_target_embedded = PositionalEmbedding(\n",
    "    sequence_length=sequence_length,\n",
    "    input_dim=max_tokens,\n",
    "    output_dim=embed_dim,\n",
    "    name=\"target_embedding\",\n",
    ") (train_source_vectors)\n",
    "\n",
    "print(\"Source embedded vectors (shape):\", train_source_embedded.shape)\n",
    "print(\"Target embedded vectors (shape):\", train_target_embedded.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Attention mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple implementation of a Transformer's attention mechanism from scratch.\n",
    "\n",
    "In practice, we could just use **tf.keras.layers.MultiHeadAttention** instead of building it from scratch. ðŸ˜’"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot-Product Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled dot product attention is calculated as follows :\n",
    "\n",
    "***Attention(Q, K, V) = softmax(QK^T / âˆš(d_k))V***\n",
    "\n",
    "Where Q, K, V are the query, key, and value matrices and d_k is the dimensionality of the key matrix.\n",
    "\n",
    "<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-19.png\" width=\"100\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "def attention_mask(nd, ns, *, dtype):\n",
    "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "    \"\"\"\n",
    "    i = tf.range(nd)[:,None]\n",
    "    j = tf.range(ns)\n",
    "    m = i >= j - ns + nd\n",
    "    return tf.cast(m, dtype)\n",
    "\n",
    "def mask_attn_weights(w):\n",
    "    # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "    _, _, nd, ns = shape_list(w)\n",
    "    b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "    b = tf.reshape(b, [1, 1, nd, ns])\n",
    "    w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "    return w\n",
    "    \n",
    "def scaled_dot_product_attention(q, k, v, use_causal_mask=False):\n",
    "    d_k = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scores = tf.matmul(q, k, transpose_b=True) # Matmul of Q and K\n",
    "    scaled_scores = scores / tf.math.sqrt(d_k) # Scale\n",
    "    if use_causal_mask:\n",
    "        scaled_scores = mask_attn_weights(scaled_scores)\n",
    "    weights = tf.nn.softmax(scaled_scores, axis=-1) # SoftMax\n",
    "    outputs = tf.matmul(weights, v) # Matmul of SoftMax and V\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the shape of our attention output just to test the function\n",
    "input = train_source_embedded\n",
    "input = tf.expand_dims(input, axis=1)\n",
    "print(\"Scaled dot product attention (shape):\", scaled_dot_product_attention(input, input, input, use_causal_mask=True).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the masking of a random tensor just to test the function\n",
    "random_tensor = tf.random.uniform(shape=(1, 1, 5, 5), minval=0, maxval=1, dtype=tf.float32)\n",
    "print(\"Masked attention weights:\", mask_attn_weights(random_tensor))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The multi-head attention is calculated as follows :\n",
    "\n",
    "***MultiHead(Q, K, V) = Concat(head_1, ..., head_h)W^O***\n",
    "\n",
    "Where head_i is the i-th attention head, W^O is the output matrix, and h is the number of attention heads.\n",
    "    \n",
    "<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-20.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.query_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(embed_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(embed_dim)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, shape=(batch_size, -1, self.num_heads, self.embed_dim // self.num_heads))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def concat_heads(self, x, batch_size):\n",
    "        x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        return tf.reshape(x, (batch_size, -1, self.embed_dim))\n",
    "\n",
    "    def call(self, q, k, v, use_causal_mask=False):\n",
    "        batch_size = tf.shape(k)[0]\n",
    "        q = self.query_dense(q)\n",
    "        k = self.key_dense(k)\n",
    "        v = self.value_dense(v)\n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        a = scaled_dot_product_attention(q, k, v, use_causal_mask)\n",
    "        a = self.concat_heads(a, batch_size)\n",
    "        c = self.combine_heads(a)\n",
    "        return c\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MultiHeadAttention, self).get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/renaudjmathieu/notebooks/main/images/chapter03_encoder-zoom.png\" width=\"400\">\n",
    "\n",
    "In the Encoder's Multi-Head Self-Attention layer ([Global self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_global_self_attention_layer)), the Encoder's input is passed to all three parameters : **Q**uery, **K**ey, and **V**alue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, dense_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.global_self_attention = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        \n",
    "    def call(self, x):\n",
    "        # Post layer normalization\n",
    "        x = self.layer_norm_1(x + self.global_self_attention(x, x, x))\n",
    "        x = self.layer_norm_2(x + self.feed_forward(x))\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/renaudjmathieu/notebooks/main/images/chapter03_decoder-zoom.png\" width=\"500\">\n",
    "\n",
    "In the Decoder's Masked Multi-Head Self-Attention layer ([Causal self-attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_causal_self_attention_layer)), the Decoder's input is passed to all three parameters : **Q**uery, **K**ey, and **V**alue.\n",
    "\n",
    "In the Decoderâ€™s Encoder-Decoder Attention layer ([Cross attention layer](https://www.tensorflow.org/text/tutorials/transformer#the_cross_attention_layer)), the outputs of the Encoder stack are passed to the **K**ey and **V**alue parameters, with the intermediate representations of the Decoder being passed to the **Q**uery parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, embed_dim, dense_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.causal_self_attention = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim)\n",
    "        self.cross_attention = MultiHeadAttention(num_heads=num_heads, embed_dim=embed_dim)\n",
    "        self.feed_forward = tf.keras.Sequential(\n",
    "            [tf.keras.layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             tf.keras.layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layer_norm_3 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def call(self, x, context):\n",
    "        # Post layer normalization\n",
    "        x = self.layer_norm_1(x + self.causal_self_attention(x, x, x, use_causal_mask=True))\n",
    "        x = self.layer_norm_2(x + self.cross_attention(q=x, k=context, v=context))\n",
    "        x = self.layer_norm_3(x + self.feed_forward(x))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://ar5iv.labs.arxiv.org/html/1706.03762/assets/Figures/ModalNet-21.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"source\")\n",
    "x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(num_heads, embed_dim, dense_dim)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"target\")\n",
    "x = PositionalEmbedding(sequence_length, max_tokens, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(num_heads, embed_dim, dense_dim)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(max_tokens, activation=\"softmax\")(x)\n",
    "\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our input pipeline using the **tf.data** API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(source, target):\n",
    "    source = source_vectorization(source)\n",
    "    target = target_vectorization(target)\n",
    "    return ({\n",
    "        \"source\": source,\n",
    "        \"target\": target[:, :-1],\n",
    "    }, target[:, 1:])\n",
    "\n",
    "def make_dataset(texts):\n",
    "    source_texts, target_texts = zip(*texts)\n",
    "    source_texts = list(source_texts)\n",
    "    target_texts = list(target_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((source_texts, target_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train)\n",
    "validation_ds = make_dataset(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the shape of the first batch of data in the dataset just to see what it looks like\n",
    "for batch in train_ds.take(1):\n",
    "    print(\"Source shape:\", batch[0][\"source\"].shape)\n",
    "    print(\"Target shape:\", batch[0][\"target\"].shape)\n",
    "    print(\"Target shape:\", batch[1].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "EPOCHS = 3\n",
    "checkpoint_filepath = '/tmp/checkpoint/'\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=3,\n",
    "    ),\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=6,\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True\n",
    "    ),\n",
    "]\n",
    "    \n",
    "transformer.fit(train_ds, \n",
    "                epochs=EPOCHS, \n",
    "                callbacks=callbacks_list,\n",
    "                validation_data=validation_ds)\n",
    "\n",
    "transformer.load_weights(checkpoint_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's translate a few random test sentences with our trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab = target_vectorization.get_vocabulary()\n",
    "target_index_lookup = dict(zip(range(len(target_vocab)), target_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = target_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_source_texts = [text[0] for text in test]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_source_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits to the following\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part03_transformer.ipynb\n",
    "\n",
    "- https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/chapter11_part04_sequence-to-sequence-learning.ipynb\n",
    "\n",
    "- https://github.com/karpathy/minGPT/blob/master/mingpt/model.py\n",
    "\n",
    "- https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "\n",
    "- https://arxiv.org/abs/1706.03762\n",
    "\n",
    "- https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "- https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-11\n",
    "\n",
    "- https://www.oreilly.com/library/view/natural-language-processing/9781098136789/ch03.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c472d3f56e1d3981a3fd37d3eee44d1ccdd21bd4124bc6c2c96c1c4dcfe0833"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
